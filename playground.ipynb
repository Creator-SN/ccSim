{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `CLS`-交互式模型训练的样例\n",
    "- model: BERT\n",
    "- dataset: CNSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fct_loss: 损失函数类型, 共有三种`MSELoss`, `BCELoss`, `CrossEntropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTS\", \"CNSTS\", model_name='bert', padding_length=150, batch_size=64, batch_size_eval=128, eval_mode='dev', task_name='STS_CNSTS_bert')\n",
    "\n",
    "for i in trainer(fct_loss='MSELoss', gpu=[0]):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `CLS`-Siamese模型训练的样例\n",
    "- model: ESIM\n",
    "- dataset: CNSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTS\", \"ASAG\", model_name=\"sbert\",  model_type=\"siamese\", padding_length=256, batch_size=8, batch_size_eval=128, eval_mode='dev', task_name='SAS_ASAG_sbert')\n",
    "\n",
    "for i in trainer():\n",
    "    a = i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `CLS`-AC模型训练的样例\n",
    "- model: ACBert\n",
    "- dataset: CNSTSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import Trainer\n",
    "from CC.loaders.acSTSLoader import WordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16000it [00:01, 10065.67it/s]\n",
      "4000it [00:00, 9745.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoModel Choose Model: acbert\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing ACBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing ACBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ACBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ACBertModel were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.0.fuse_layernorm.bias', 'bert.embeddings.position_ids', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.word_transform.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/lpc/miniconda3/envs/pcpower/lib/python3.9/site-packages/transformers/modeling_utils.py:768: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Train: 1/30: 100%|██████████| 250/250 [00:47<00:00,  5.22it/s, train_acc=0.774, train_loss=0.153]\n",
      "Eval: 1: 100%|██████████| 32/32 [00:04<00:00,  7.49it/s, eval_acc=0.851, eval_loss=0.107]\n",
      "Train: 2/30: 100%|██████████| 250/250 [00:47<00:00,  5.23it/s, train_acc=0.865, train_loss=0.0983]\n",
      "Eval: 2: 100%|██████████| 32/32 [00:04<00:00,  7.48it/s, eval_acc=0.862, eval_loss=0.101] \n",
      "Train: 3/30: 100%|██████████| 250/250 [00:47<00:00,  5.23it/s, train_acc=0.898, train_loss=0.0761]\n",
      "Eval: 3: 100%|██████████| 32/32 [00:04<00:00,  7.50it/s, eval_acc=0.85, eval_loss=0.116] \n",
      "Train: 4/30: 100%|██████████| 250/250 [00:47<00:00,  5.22it/s, train_acc=0.908, train_loss=0.0694]\n",
      "Eval: 4: 100%|██████████| 32/32 [00:04<00:00,  7.47it/s, eval_acc=0.862, eval_loss=0.106]\n",
      "Train: 5/30: 100%|██████████| 250/250 [00:47<00:00,  5.22it/s, train_acc=0.933, train_loss=0.0525]\n",
      "Eval: 5: 100%|██████████| 32/32 [00:04<00:00,  7.46it/s, eval_acc=0.875, eval_loss=0.0991]\n",
      "Train: 6/30: 100%|██████████| 250/250 [00:47<00:00,  5.22it/s, train_acc=0.945, train_loss=0.0436]\n",
      "Eval: 6: 100%|██████████| 32/32 [00:04<00:00,  7.48it/s, eval_acc=0.879, eval_loss=0.101] \n",
      "Train: 7/30: 100%|██████████| 250/250 [00:47<00:00,  5.22it/s, train_acc=0.952, train_loss=0.0382]\n",
      "Eval: 7: 100%|██████████| 32/32 [00:04<00:00,  7.47it/s, eval_acc=0.874, eval_loss=0.101] \n",
      "Train: 8/30: 100%|██████████| 250/250 [00:47<00:00,  5.22it/s, train_acc=0.955, train_loss=0.0358]\n",
      "Eval: 8: 100%|██████████| 32/32 [00:04<00:00,  7.46it/s, eval_acc=0.872, eval_loss=0.108]\n",
      "Train: 9/30: 100%|██████████| 250/250 [00:47<00:00,  5.22it/s, train_acc=0.963, train_loss=0.0298]\n",
      "Eval: 9: 100%|██████████| 32/32 [00:04<00:00,  7.47it/s, eval_acc=0.869, eval_loss=0.11] \n",
      "Train: 10/30: 100%|██████████| 250/250 [00:47<00:00,  5.22it/s, train_acc=0.963, train_loss=0.029] \n",
      "Eval: 10: 100%|██████████| 32/32 [00:04<00:00,  7.46it/s, eval_acc=0.877, eval_loss=0.108]\n",
      "Train: 11/30: 100%|██████████| 250/250 [00:47<00:00,  5.22it/s, train_acc=0.966, train_loss=0.0273]\n",
      "Eval: 11: 100%|██████████| 32/32 [00:04<00:00,  7.47it/s, eval_acc=0.875, eval_loss=0.106] \n",
      "Train: 12/30: 100%|██████████| 250/250 [00:47<00:00,  5.21it/s, train_acc=0.974, train_loss=0.0216]\n",
      "Eval: 12: 100%|██████████| 32/32 [00:04<00:00,  7.46it/s, eval_acc=0.877, eval_loss=0.108]\n",
      "Train: 13/30: 100%|██████████| 250/250 [00:47<00:00,  5.22it/s, train_acc=0.973, train_loss=0.0219]\n",
      "Eval: 13: 100%|██████████| 32/32 [00:04<00:00,  7.46it/s, eval_acc=0.875, eval_loss=0.106]\n",
      "Train: 14/30: 100%|██████████| 250/250 [00:47<00:00,  5.22it/s, train_acc=0.973, train_loss=0.0222]\n",
      "Eval: 14: 100%|██████████| 32/32 [00:04<00:00,  7.47it/s, eval_acc=0.87, eval_loss=0.112] \n",
      "Train: 15/30: 100%|██████████| 250/250 [00:47<00:00,  5.22it/s, train_acc=0.974, train_loss=0.0212]\n",
      "Eval: 15: 100%|██████████| 32/32 [00:04<00:00,  7.45it/s, eval_acc=0.873, eval_loss=0.111]\n",
      "Train: 16/30:  80%|████████  | 200/250 [00:38<00:09,  5.22it/s, train_acc=0.978, train_loss=0.0182]"
     ]
    }
   ],
   "source": [
    "tokenizer = WordTokenizer(vocab_file='./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"ACSTS\", \"CNSTSAC\", model_name=\"acbert\",  padding_length=150, batch_size=64, batch_size_eval=128, eval_mode='dev', task_name='CLS_CNSTSAC_acbert')\n",
    "\n",
    "for i in trainer(gpu=[0]):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `CLS`-交互式模型预测的样例\n",
    "- model: BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.predictor import Predictor\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "predictor = Predictor(tokenizer, model_name=\"bert\",  padding_length=256, resume_path='./save_model/Sim/bert/bert_7500.pth', batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in predictor([['你好', '我很好']]):\n",
    "    a = i\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `SAS训练`交互式模型的样例\n",
    "- model: BERT\n",
    "- dataset: CNSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.sas_trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTSX\", \"CNSTSX\", model_name='bert', padding_length=150, batch_size=16, batch_size_eval=512, eval_mode='dev', task_name='SAS_CNSTSX_bert')\n",
    "\n",
    "for i in trainer(fct_loss='MSELoss'):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `SAS训练`Siamese模型的样例\n",
    "- model: SBERT\n",
    "- dataset: CNSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.sas_trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTSX\", \"CNSTSX\", model_name='bimpm', model_type='siamese', padding_length=100, batch_size=32, batch_size_eval=512, eval_mode='dev', task_name='SAS_CNSTSX_bimpm')\n",
    "\n",
    "for i in trainer(fct_loss='MSELoss', lr=1e-3):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `SAS训练`MSIM模型的样例\n",
    "- model: MSIM\n",
    "- dataset: CNSTSX\n",
    "\n",
    "最佳参数:\n",
    "- SAS: A + B + 0.2(C + D)\n",
    "- ASAG: A + B + 0.2(C + D)\n",
    "- SFR: 0.1A + B + 0.2(C + D)\n",
    "- CNSTS: A + B + 0.2(C + D)\n",
    "\n",
    "FewShot最佳参数:\n",
    "- SAS: A + B + 0.2(C + D)\n",
    "- ASAG: 0.1A + 0.8B + 0.2(C + D)\n",
    "\n",
    "Fewshot Prompt Fine-tuning时 ASAG数据集需要1e-4学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.sas_trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTSX\", \"SFR\", model_name='msim', model_type='siamese', padding_length=256, batch_size=8, batch_size_eval=256, eval_mode='dev', task_name='SAS_SFR_msim')\n",
    "\n",
    "for i in trainer(fct_loss='MSELoss'):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `SIMCSE` 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.cl_trainer import CLTrainer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = CLTrainer(tokenizer, \"SIMCSE_STS\", \"SAS\", model_name='simcse', model_type='siamese', padding_length=256, batch_size=16, batch_size_eval=256, eval_mode='dev', task_name='SIMCSE_SAS')\n",
    "\n",
    "for i in trainer(fct_loss='MSELoss'):\n",
    "    a = i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.prompt_trainer import PromptTrainer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = PromptTrainer(tokenizer, \"SASPrompt\", \"ASAGPrompt\", model_name='bertlm', padding_length=256, batch_size=16, batch_size_eval=256, eval_mode='dev', task_name='Prompt_ASAG')\n",
    "\n",
    "for i in trainer():\n",
    "    a = i\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pcpower')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e1e097b6c3c5a2a39328ddbc7de6327b7bd71c15618bc750f041eecacee4167"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
