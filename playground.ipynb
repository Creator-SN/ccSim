{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 交互式模型训练的样例\n",
    "- model: BERT\n",
    "- dataset: CNSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fct_loss: 损失函数类型, 共有三种`MSELoss`, `BCELoss`, `CrossEntropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTS\", \"CNSTS\", padding_length=150, batch_size=64, batch_size_eval=128, eval_mode='dev')\n",
    "\n",
    "for i in trainer(fct_loss='BCELoss'):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Siamese模型训练的样例\n",
    "- model: ESIM\n",
    "- dataset: CNSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTS\", \"CNSTS\", model_name=\"esim\",  model_type=\"siamese\", padding_length=150, batch_size=64, batch_size_eval=128, eval_mode='dev', task_name='ESIM_Sim')\n",
    "\n",
    "for i in trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 交互式模型SAS训练的样例\n",
    "- model: BERT\n",
    "- dataset: CNSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.sas_trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpc/miniconda3/envs/pcpower/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1642: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "100%|██████████| 409/409 [00:00<00:00, 16245.14it/s]\n",
      "100%|██████████| 176/176 [00:00<00:00, 16146.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoModel Choose Model: bert\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]/home/lpc/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/lpc/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:117: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Train: 1/30: 100%|██████████| 26/26 [00:15<00:00,  1.72it/s, Pearson=-.116, R=-.116, RMSE=0.374, Spearman=-.108, train_error=0.317, train_loss=0.141]\n",
      "Eval: 1: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s, Pearson=0.386, R=0.386, RMSE=0.326, Spearman=0.351, eval_error=0.272, eval_loss=0.107]\n",
      "Train: 2/30: 100%|██████████| 26/26 [00:08<00:00,  3.15it/s, Pearson=0.409, R=0.409, RMSE=0.321, Spearman=0.379, train_error=0.271, train_loss=0.104] \n",
      "Eval: 2: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s, Pearson=0.692, R=0.692, RMSE=0.273, Spearman=0.696, eval_error=0.226, eval_loss=0.0745]\n",
      "Train: 3/30: 100%|██████████| 26/26 [00:08<00:00,  3.11it/s, Pearson=0.775, R=0.775, RMSE=0.224, Spearman=0.764, train_error=0.183, train_loss=0.0498]\n",
      "Eval: 3: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, Pearson=0.819, R=0.819, RMSE=0.194, Spearman=0.852, eval_error=0.157, eval_loss=0.0377]\n",
      "Train: 4/30: 100%|██████████| 26/26 [00:08<00:00,  3.06it/s, Pearson=0.866, R=0.866, RMSE=0.175, Spearman=0.859, train_error=0.147, train_loss=0.0305]\n",
      "Eval: 4: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, Pearson=0.808, R=0.808, RMSE=0.246, Spearman=0.842, eval_error=0.191, eval_loss=0.0608]\n",
      "Train: 5/30: 100%|██████████| 26/26 [00:08<00:00,  3.02it/s, Pearson=0.907, R=0.907, RMSE=0.147, Spearman=0.906, train_error=0.123, train_loss=0.0213]\n",
      "Eval: 5: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s, Pearson=0.855, R=0.855, RMSE=0.184, Spearman=0.878, eval_error=0.143, eval_loss=0.0338]\n",
      "Train: 6/30: 100%|██████████| 26/26 [00:08<00:00,  3.12it/s, Pearson=0.941, R=0.941, RMSE=0.118, Spearman=0.934, train_error=0.0941, train_loss=0.0138]\n",
      "Eval: 6: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s, Pearson=0.868, R=0.868, RMSE=0.178, Spearman=0.891, eval_error=0.124, eval_loss=0.0316]\n",
      "Train: 7/30: 100%|██████████| 26/26 [00:08<00:00,  3.15it/s, Pearson=0.939, R=0.939, RMSE=0.12, Spearman=0.935, train_error=0.0918, train_loss=0.0148]  \n",
      "Eval: 7: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, Pearson=0.813, R=0.813, RMSE=0.239, Spearman=0.849, eval_error=0.171, eval_loss=0.057]\n",
      "Train: 8/30: 100%|██████████| 26/26 [00:08<00:00,  3.06it/s, Pearson=0.946, R=0.946, RMSE=0.114, Spearman=0.941, train_error=0.0856, train_loss=0.0128]\n",
      "Eval: 8: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s, Pearson=0.805, R=0.805, RMSE=0.252, Spearman=0.856, eval_error=0.177, eval_loss=0.0637]\n",
      "Train: 9/30: 100%|██████████| 26/26 [00:08<00:00,  3.04it/s, Pearson=0.92, R=0.92, RMSE=0.138, Spearman=0.909, train_error=0.0982, train_loss=0.0187]   \n",
      "Eval: 9: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s, Pearson=0.849, R=0.849, RMSE=0.179, Spearman=0.859, eval_error=0.125, eval_loss=0.0319]\n",
      "Train: 10/30: 100%|██████████| 26/26 [00:08<00:00,  3.15it/s, Pearson=0.906, R=0.906, RMSE=0.149, Spearman=0.902, train_error=0.107, train_loss=0.0219]\n",
      "Eval: 10: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s, Pearson=0.84, R=0.84, RMSE=0.184, Spearman=0.858, eval_error=0.125, eval_loss=0.0339]\n",
      "Train: 11/30: 100%|██████████| 26/26 [00:08<00:00,  3.03it/s, Pearson=0.954, R=0.954, RMSE=0.105, Spearman=0.948, train_error=0.0743, train_loss=0.0108]  \n",
      "Eval: 11: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, Pearson=0.882, R=0.882, RMSE=0.173, Spearman=0.893, eval_error=0.116, eval_loss=0.0299]\n",
      "Train: 12/30: 100%|██████████| 26/26 [00:08<00:00,  3.09it/s, Pearson=0.954, R=0.954, RMSE=0.106, Spearman=0.946, train_error=0.0751, train_loss=0.0116]\n",
      "Eval: 12: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, Pearson=0.876, R=0.876, RMSE=0.17, Spearman=0.888, eval_error=0.108, eval_loss=0.0291]\n",
      "Train: 13/30: 100%|██████████| 26/26 [00:08<00:00,  3.10it/s, Pearson=0.966, R=0.966, RMSE=0.0911, Spearman=0.958, train_error=0.0648, train_loss=0.00829]\n",
      "Eval: 13: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s, Pearson=0.87, R=0.87, RMSE=0.167, Spearman=0.882, eval_error=0.112, eval_loss=0.0279]\n",
      "Train: 14/30: 100%|██████████| 26/26 [00:08<00:00,  3.14it/s, Pearson=0.979, R=0.979, RMSE=0.0713, Spearman=0.973, train_error=0.0498, train_loss=0.00504]\n",
      "Eval: 14: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, Pearson=0.887, R=0.887, RMSE=0.157, Spearman=0.894, eval_error=0.103, eval_loss=0.0245]\n",
      "Train: 15/30: 100%|██████████| 26/26 [00:08<00:00,  3.00it/s, Pearson=0.976, R=0.976, RMSE=0.0765, Spearman=0.969, train_error=0.0533, train_loss=0.0058] \n",
      "Eval: 15: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s, Pearson=0.877, R=0.877, RMSE=0.165, Spearman=0.89, eval_error=0.1, eval_loss=0.0272]\n",
      "Train: 16/30: 100%|██████████| 26/26 [00:08<00:00,  3.07it/s, Pearson=0.978, R=0.978, RMSE=0.0735, Spearman=0.971, train_error=0.0503, train_loss=0.00533]\n",
      "Eval: 16: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s, Pearson=0.9, R=0.9, RMSE=0.154, Spearman=0.911, eval_error=0.105, eval_loss=0.0237]\n",
      "Train: 17/30: 100%|██████████| 26/26 [00:08<00:00,  3.14it/s, Pearson=0.98, R=0.98, RMSE=0.0693, Spearman=0.973, train_error=0.0463, train_loss=0.00473]  \n",
      "Eval: 17: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s, Pearson=0.889, R=0.889, RMSE=0.157, Spearman=0.901, eval_error=0.0961, eval_loss=0.0245]\n",
      "Train: 18/30: 100%|██████████| 26/26 [00:08<00:00,  3.09it/s, Pearson=0.985, R=0.985, RMSE=0.0601, Spearman=0.975, train_error=0.0396, train_loss=0.00356]\n",
      "Eval: 18: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, Pearson=0.885, R=0.885, RMSE=0.16, Spearman=0.897, eval_error=0.103, eval_loss=0.0257]\n",
      "Train: 19/30: 100%|██████████| 26/26 [00:08<00:00,  2.92it/s, Pearson=0.983, R=0.983, RMSE=0.0637, Spearman=0.975, train_error=0.0431, train_loss=0.004]  \n",
      "Eval: 19: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s, Pearson=0.884, R=0.884, RMSE=0.161, Spearman=0.901, eval_error=0.0938, eval_loss=0.0259]\n",
      "Train: 20/30: 100%|██████████| 26/26 [00:08<00:00,  3.09it/s, Pearson=0.984, R=0.984, RMSE=0.0625, Spearman=0.974, train_error=0.0399, train_loss=0.00385]\n",
      "Eval: 20: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s, Pearson=0.89, R=0.89, RMSE=0.154, Spearman=0.901, eval_error=0.0933, eval_loss=0.0239]\n",
      "Train: 21/30: 100%|██████████| 26/26 [00:08<00:00,  3.02it/s, Pearson=0.988, R=0.988, RMSE=0.0546, Spearman=0.977, train_error=0.0367, train_loss=0.00293]\n",
      "Eval: 21: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, Pearson=0.892, R=0.892, RMSE=0.154, Spearman=0.902, eval_error=0.0943, eval_loss=0.0238]\n",
      "Train: 22/30: 100%|██████████| 26/26 [00:08<00:00,  3.01it/s, Pearson=0.986, R=0.986, RMSE=0.0583, Spearman=0.976, train_error=0.0388, train_loss=0.00335]\n",
      "Eval: 22: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s, Pearson=0.896, R=0.896, RMSE=0.15, Spearman=0.905, eval_error=0.093, eval_loss=0.0226]\n",
      "Train: 23/30: 100%|██████████| 26/26 [00:08<00:00,  3.16it/s, Pearson=0.988, R=0.988, RMSE=0.0535, Spearman=0.978, train_error=0.0354, train_loss=0.00283]\n",
      "Eval: 23: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s, Pearson=0.884, R=0.884, RMSE=0.166, Spearman=0.898, eval_error=0.103, eval_loss=0.0277]\n",
      "Train: 24/30: 100%|██████████| 26/26 [00:08<00:00,  3.05it/s, Pearson=0.986, R=0.986, RMSE=0.0577, Spearman=0.976, train_error=0.0376, train_loss=0.0033] \n",
      "Eval: 24: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, Pearson=0.884, R=0.884, RMSE=0.166, Spearman=0.898, eval_error=0.105, eval_loss=0.0274]\n",
      "Train: 25/30: 100%|██████████| 26/26 [00:08<00:00,  3.04it/s, Pearson=0.985, R=0.985, RMSE=0.0603, Spearman=0.977, train_error=0.0384, train_loss=0.00359]\n",
      "Eval: 25: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, Pearson=0.881, R=0.881, RMSE=0.173, Spearman=0.899, eval_error=0.109, eval_loss=0.0299]\n",
      "Train: 26/30: 100%|██████████| 26/26 [00:08<00:00,  3.12it/s, Pearson=0.985, R=0.985, RMSE=0.061, Spearman=0.975, train_error=0.0389, train_loss=0.00366] \n",
      "Eval: 26: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s, Pearson=0.871, R=0.871, RMSE=0.188, Spearman=0.89, eval_error=0.126, eval_loss=0.0353]\n",
      "Train: 27/30: 100%|██████████| 26/26 [00:08<00:00,  3.17it/s, Pearson=0.988, R=0.988, RMSE=0.0553, Spearman=0.979, train_error=0.0366, train_loss=0.00303]\n",
      "Eval: 27: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, Pearson=0.859, R=0.859, RMSE=0.204, Spearman=0.887, eval_error=0.139, eval_loss=0.0415]\n",
      "Train: 28/30: 100%|██████████| 26/26 [00:08<00:00,  3.08it/s, Pearson=0.979, R=0.979, RMSE=0.0721, Spearman=0.967, train_error=0.0459, train_loss=0.00526]\n",
      "Eval: 28: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s, Pearson=0.855, R=0.855, RMSE=0.213, Spearman=0.884, eval_error=0.149, eval_loss=0.0456]\n",
      "Train: 29/30: 100%|██████████| 26/26 [00:08<00:00,  3.02it/s, Pearson=0.969, R=0.969, RMSE=0.0869, Spearman=0.962, train_error=0.0524, train_loss=0.00744]\n",
      "Eval: 29: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s, Pearson=0.854, R=0.854, RMSE=0.202, Spearman=0.871, eval_error=0.134, eval_loss=0.0409]\n",
      "Train: 30/30: 100%|██████████| 26/26 [00:08<00:00,  3.14it/s, Pearson=0.977, R=0.977, RMSE=0.0746, Spearman=0.969, train_error=0.051, train_loss=0.0055]  \n",
      "Eval: 30: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, Pearson=0.847, R=0.847, RMSE=0.212, Spearman=0.871, eval_error=0.146, eval_loss=0.0449]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTSX\", \"SAS\", model_name='bert', padding_length=256, batch_size=16, batch_size_eval=512, eval_mode='dev', task_name='SAS_SAS_BERT')\n",
    "\n",
    "for i in trainer(fct_loss='MSELoss'):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 交互式模型SAS训练的样例\n",
    "- model: MSIM\n",
    "- dataset: CNSTSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.sas_trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpc/miniconda3/envs/pcpower/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1642: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "100%|██████████| 409/409 [00:00<00:00, 16449.20it/s]\n",
      "100%|██████████| 176/176 [00:00<00:00, 15784.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoModel Choose Model: x\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]/home/lpc/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/lpc/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:117: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Train: 1/30: 100%|██████████| 26/26 [00:18<00:00,  1.39it/s, Pearson=0.0784, R=0.0784, RMSE=0.849, Spearman=0.109, train_error=0.762, train_loss=0.715]\n",
      "Eval: 1: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it, Pearson=-.318, R=-.318, RMSE=0.538, Spearman=-.297, eval_error=0.437, eval_loss=0.29]\n",
      "Train: 2/30: 100%|██████████| 26/26 [00:11<00:00,  2.25it/s, Pearson=0.341, R=0.341, RMSE=0.339, Spearman=0.356, train_error=0.262, train_loss=0.113]   \n",
      "Eval: 2: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it, Pearson=0.786, R=0.786, RMSE=0.23, Spearman=0.795, eval_error=0.18, eval_loss=0.0529]\n",
      "Train: 3/30: 100%|██████████| 26/26 [00:12<00:00,  2.16it/s, Pearson=0.829, R=0.829, RMSE=0.195, Spearman=0.831, train_error=0.15, train_loss=0.0378] \n",
      "Eval: 3: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it, Pearson=0.822, R=0.822, RMSE=0.221, Spearman=0.864, eval_error=0.168, eval_loss=0.049]\n",
      "Train: 4/30: 100%|██████████| 26/26 [00:12<00:00,  2.15it/s, Pearson=0.916, R=0.916, RMSE=0.14, Spearman=0.922, train_error=0.108, train_loss=0.0194] \n",
      "Eval: 4: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it, Pearson=0.839, R=0.839, RMSE=0.199, Spearman=0.871, eval_error=0.149, eval_loss=0.0396]\n",
      "Train: 5/30: 100%|██████████| 26/26 [00:12<00:00,  2.13it/s, Pearson=0.928, R=0.928, RMSE=0.13, Spearman=0.93, train_error=0.105, train_loss=0.0169]   \n",
      "Eval: 5: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it, Pearson=0.844, R=0.844, RMSE=0.183, Spearman=0.871, eval_error=0.137, eval_loss=0.0335]\n",
      "Train: 6/30: 100%|██████████| 26/26 [00:12<00:00,  2.12it/s, Pearson=0.927, R=0.927, RMSE=0.131, Spearman=0.928, train_error=0.104, train_loss=0.0171]  \n",
      "Eval: 6: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it, Pearson=0.833, R=0.833, RMSE=0.213, Spearman=0.849, eval_error=0.167, eval_loss=0.0456]\n",
      "Train: 7/30: 100%|██████████| 26/26 [00:12<00:00,  2.11it/s, Pearson=0.923, R=0.923, RMSE=0.136, Spearman=0.919, train_error=0.11, train_loss=0.0185] \n",
      "Eval: 7: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it, Pearson=0.811, R=0.811, RMSE=0.199, Spearman=0.824, eval_error=0.139, eval_loss=0.0395]\n",
      "Train: 8/30: 100%|██████████| 26/26 [00:12<00:00,  2.10it/s, Pearson=0.886, R=0.886, RMSE=0.163, Spearman=0.884, train_error=0.129, train_loss=0.0265]  \n",
      "Eval: 8: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it, Pearson=0.842, R=0.842, RMSE=0.196, Spearman=0.869, eval_error=0.146, eval_loss=0.0385]\n",
      "Train: 9/30: 100%|██████████| 26/26 [00:12<00:00,  2.11it/s, Pearson=0.897, R=0.897, RMSE=0.156, Spearman=0.908, train_error=0.123, train_loss=0.0246] \n",
      "Eval: 9: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it, Pearson=0.825, R=0.825, RMSE=0.194, Spearman=0.844, eval_error=0.152, eval_loss=0.0376]\n",
      "Train: 10/30: 100%|██████████| 26/26 [00:12<00:00,  2.15it/s, Pearson=0.929, R=0.929, RMSE=0.13, Spearman=0.932, train_error=0.105, train_loss=0.017]  \n",
      "Eval: 10: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it, Pearson=0.891, R=0.891, RMSE=0.185, Spearman=0.896, eval_error=0.142, eval_loss=0.0342]\n",
      "Train: 11/30: 100%|██████████| 26/26 [00:11<00:00,  2.19it/s, Pearson=0.935, R=0.935, RMSE=0.126, Spearman=0.935, train_error=0.0967, train_loss=0.0156]\n",
      "Eval: 11: 100%|██████████| 1/1 [00:01<00:00,  1.66s/it, Pearson=0.887, R=0.887, RMSE=0.168, Spearman=0.898, eval_error=0.124, eval_loss=0.0282]\n",
      "Train: 12/30: 100%|██████████| 26/26 [00:11<00:00,  2.24it/s, Pearson=0.941, R=0.941, RMSE=0.12, Spearman=0.942, train_error=0.094, train_loss=0.0144]   \n",
      "Eval: 12: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it, Pearson=0.878, R=0.878, RMSE=0.199, Spearman=0.897, eval_error=0.143, eval_loss=0.0396]\n",
      "Train: 13/30: 100%|██████████| 26/26 [00:11<00:00,  2.17it/s, Pearson=0.976, R=0.976, RMSE=0.0755, Spearman=0.971, train_error=0.0564, train_loss=0.00564]\n",
      "Eval: 13: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it, Pearson=0.907, R=0.907, RMSE=0.152, Spearman=0.918, eval_error=0.107, eval_loss=0.023]\n",
      "Train: 14/30: 100%|██████████| 26/26 [00:12<00:00,  2.15it/s, Pearson=0.99, R=0.99, RMSE=0.0489, Spearman=0.983, train_error=0.0369, train_loss=0.00238]  \n",
      "Eval: 14: 100%|██████████| 1/1 [00:01<00:00,  1.41s/it, Pearson=0.906, R=0.906, RMSE=0.151, Spearman=0.916, eval_error=0.105, eval_loss=0.0229]\n",
      "Train: 15/30: 100%|██████████| 26/26 [00:12<00:00,  2.13it/s, Pearson=0.992, R=0.992, RMSE=0.0434, Spearman=0.984, train_error=0.0314, train_loss=0.00191]\n",
      "Eval: 15: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it, Pearson=0.906, R=0.906, RMSE=0.163, Spearman=0.919, eval_error=0.116, eval_loss=0.0265]\n",
      "Train: 16/30: 100%|██████████| 26/26 [00:12<00:00,  2.13it/s, Pearson=0.994, R=0.994, RMSE=0.0386, Spearman=0.986, train_error=0.0294, train_loss=0.00149]\n",
      "Eval: 16: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it, Pearson=0.904, R=0.904, RMSE=0.162, Spearman=0.916, eval_error=0.113, eval_loss=0.0263]\n",
      "Train: 17/30: 100%|██████████| 26/26 [00:12<00:00,  2.05it/s, Pearson=0.991, R=0.991, RMSE=0.0455, Spearman=0.983, train_error=0.0355, train_loss=0.00205]\n",
      "Eval: 17: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it, Pearson=0.899, R=0.899, RMSE=0.171, Spearman=0.913, eval_error=0.119, eval_loss=0.0294]\n",
      "Train: 18/30: 100%|██████████| 26/26 [00:12<00:00,  2.08it/s, Pearson=0.989, R=0.989, RMSE=0.0515, Spearman=0.982, train_error=0.0384, train_loss=0.00264]\n",
      "Eval: 18: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it, Pearson=0.897, R=0.897, RMSE=0.182, Spearman=0.91, eval_error=0.128, eval_loss=0.0331]\n",
      "Train: 19/30: 100%|██████████| 26/26 [00:12<00:00,  2.12it/s, Pearson=0.987, R=0.987, RMSE=0.0566, Spearman=0.98, train_error=0.0449, train_loss=0.0032]  \n",
      "Eval: 19: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it, Pearson=0.89, R=0.89, RMSE=0.176, Spearman=0.902, eval_error=0.119, eval_loss=0.0308]\n",
      "Train: 20/30: 100%|██████████| 26/26 [00:11<00:00,  2.17it/s, Pearson=0.984, R=0.984, RMSE=0.0623, Spearman=0.977, train_error=0.048, train_loss=0.00383] \n",
      "Eval: 20: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it, Pearson=0.899, R=0.899, RMSE=0.163, Spearman=0.909, eval_error=0.112, eval_loss=0.0265]\n",
      "Train: 21/30: 100%|██████████| 26/26 [00:11<00:00,  2.23it/s, Pearson=0.979, R=0.979, RMSE=0.072, Spearman=0.973, train_error=0.0566, train_loss=0.00516] \n",
      "Eval: 21: 100%|██████████| 1/1 [00:01<00:00,  1.73s/it, Pearson=0.904, R=0.904, RMSE=0.155, Spearman=0.917, eval_error=0.111, eval_loss=0.0239]\n",
      "Train: 22/30: 100%|██████████| 26/26 [00:11<00:00,  2.26it/s, Pearson=0.977, R=0.977, RMSE=0.0747, Spearman=0.972, train_error=0.059, train_loss=0.00559] \n",
      "Eval: 22: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it, Pearson=0.888, R=0.888, RMSE=0.159, Spearman=0.899, eval_error=0.105, eval_loss=0.0252]\n",
      "Train: 23/30: 100%|██████████| 26/26 [00:12<00:00,  2.15it/s, Pearson=0.982, R=0.982, RMSE=0.0668, Spearman=0.976, train_error=0.0515, train_loss=0.00447]\n",
      "Eval: 23: 100%|██████████| 1/1 [00:01<00:00,  1.40s/it, Pearson=0.899, R=0.899, RMSE=0.156, Spearman=0.907, eval_error=0.117, eval_loss=0.0243]\n",
      "Train: 24/30: 100%|██████████| 26/26 [00:12<00:00,  2.16it/s, Pearson=0.98, R=0.98, RMSE=0.0696, Spearman=0.972, train_error=0.0518, train_loss=0.0048]   \n",
      "Eval: 24: 100%|██████████| 1/1 [00:01<00:00,  1.40s/it, Pearson=0.899, R=0.899, RMSE=0.161, Spearman=0.907, eval_error=0.119, eval_loss=0.0258]\n",
      "Train: 25/30: 100%|██████████| 26/26 [00:12<00:00,  2.10it/s, Pearson=0.971, R=0.971, RMSE=0.0841, Spearman=0.967, train_error=0.0663, train_loss=0.00703]\n",
      "Eval: 25: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it, Pearson=0.889, R=0.889, RMSE=0.196, Spearman=0.9, eval_error=0.147, eval_loss=0.0383]\n",
      "Train: 26/30: 100%|██████████| 26/26 [00:12<00:00,  2.08it/s, Pearson=0.972, R=0.972, RMSE=0.0821, Spearman=0.966, train_error=0.0622, train_loss=0.00672]\n",
      "Eval: 26: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it, Pearson=0.887, R=0.887, RMSE=0.2, Spearman=0.897, eval_error=0.154, eval_loss=0.0399]\n",
      "Train: 27/30: 100%|██████████| 26/26 [00:12<00:00,  2.07it/s, Pearson=0.969, R=0.969, RMSE=0.0875, Spearman=0.967, train_error=0.0661, train_loss=0.00764]\n",
      "Eval: 27: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it, Pearson=0.899, R=0.899, RMSE=0.193, Spearman=0.91, eval_error=0.153, eval_loss=0.0374]\n",
      "Train: 28/30: 100%|██████████| 26/26 [00:12<00:00,  2.08it/s, Pearson=0.977, R=0.977, RMSE=0.0751, Spearman=0.971, train_error=0.0575, train_loss=0.00568]\n",
      "Eval: 28: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it, Pearson=0.885, R=0.885, RMSE=0.2, Spearman=0.899, eval_error=0.148, eval_loss=0.04]\n",
      "Train: 29/30: 100%|██████████| 26/26 [00:12<00:00,  2.12it/s, Pearson=0.959, R=0.959, RMSE=0.0988, Spearman=0.956, train_error=0.0766, train_loss=0.00967]\n",
      "Eval: 29: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it, Pearson=0.915, R=0.915, RMSE=0.183, Spearman=0.921, eval_error=0.14, eval_loss=0.0333]\n",
      "Train: 30/30: 100%|██████████| 26/26 [00:12<00:00,  2.15it/s, Pearson=0.97, R=0.97, RMSE=0.0848, Spearman=0.968, train_error=0.0644, train_loss=0.00715]  \n",
      "Eval: 30: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it, Pearson=0.88, R=0.88, RMSE=0.162, Spearman=0.89, eval_error=0.109, eval_loss=0.0263]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTSX\", \"SAS\", model_name='msim', model_type='siamese', padding_length=256, batch_size=16, batch_size_eval=256, eval_mode='dev', task_name='SAS_SAS_MSIM')\n",
    "\n",
    "for i in trainer(fct_loss='MSELoss'):\n",
    "    a = i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pcpower')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e1e097b6c3c5a2a39328ddbc7de6327b7bd71c15618bc750f041eecacee4167"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
