{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 交互式模型训练的样例\n",
    "- model: BERT\n",
    "- dataset: CNSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fct_loss: 损失函数类型, 共有三种`MSELoss`, `BCELoss`, `CrossEntropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTS\", \"CNSTS\", padding_length=150, batch_size=64, batch_size_eval=128, eval_mode='dev')\n",
    "\n",
    "for i in trainer(fct_loss='BCELoss'):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Siamese模型训练的样例\n",
    "- model: ESIM\n",
    "- dataset: CNSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTS\", \"CNSTS\", model_name=\"esim\",  model_type=\"siamese\", padding_length=150, batch_size=64, batch_size_eval=128, eval_mode='dev', task_name='ESIM_Sim')\n",
    "\n",
    "for i in trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 交互式模型SAS训练的样例\n",
    "- model: BERT\n",
    "- dataset: CNSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.sas_trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpc/miniconda3/envs/pcpower/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1642: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoModel Choose Model: bert\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/lpc/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/lpc/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:117: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Train: 1/30: 100%|██████████| 250/250 [01:48<00:00,  2.31it/s, Pearson=0.626, R=0.626, RMSE=0.39, Spearman=0.621, train_error=0.31, train_loss=0.152]     \n",
      "Eval: 1: 100%|██████████| 32/32 [00:08<00:00,  3.69it/s, Pearson=0.75, R=0.75, RMSE=0.335, Spearman=0.743, eval_error=0.202, eval_loss=0.111]  \n",
      "Train: 2/30: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s, Pearson=0.786, R=0.786, RMSE=0.309, Spearman=0.756, train_error=0.19, train_loss=0.0955] \n",
      "Eval: 2: 100%|██████████| 32/32 [00:08<00:00,  3.78it/s, Pearson=0.764, R=0.764, RMSE=0.336, Spearman=0.762, eval_error=0.167, eval_loss=0.112]\n",
      "Train: 3/30: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s, Pearson=0.839, R=0.839, RMSE=0.272, Spearman=0.792, train_error=0.144, train_loss=0.0738]\n",
      "Eval: 3: 100%|██████████| 32/32 [00:08<00:00,  3.76it/s, Pearson=0.777, R=0.777, RMSE=0.319, Spearman=0.76, eval_error=0.165, eval_loss=0.1]    \n",
      "Train: 4/30: 100%|██████████| 250/250 [01:41<00:00,  2.45it/s, Pearson=0.869, R=0.869, RMSE=0.247, Spearman=0.811, train_error=0.12, train_loss=0.0611] \n",
      "Eval: 4: 100%|██████████| 32/32 [00:08<00:00,  3.77it/s, Pearson=0.78, R=0.78, RMSE=0.318, Spearman=0.766, eval_error=0.156, eval_loss=0.101]   \n",
      "Train: 5/30: 100%|██████████| 250/250 [01:42<00:00,  2.45it/s, Pearson=0.896, R=0.896, RMSE=0.222, Spearman=0.822, train_error=0.095, train_loss=0.0492] \n",
      "Eval: 5: 100%|██████████| 32/32 [00:08<00:00,  3.75it/s, Pearson=0.765, R=0.765, RMSE=0.335, Spearman=0.754, eval_error=0.147, eval_loss=0.11] \n",
      "Train: 6/30: 100%|██████████| 250/250 [01:42<00:00,  2.45it/s, Pearson=0.908, R=0.908, RMSE=0.21, Spearman=0.829, train_error=0.0851, train_loss=0.044]  \n",
      "Eval: 6: 100%|██████████| 32/32 [00:08<00:00,  3.72it/s, Pearson=0.774, R=0.774, RMSE=0.324, Spearman=0.766, eval_error=0.152, eval_loss=0.104] \n",
      "Train: 7/30: 100%|██████████| 250/250 [01:41<00:00,  2.45it/s, Pearson=0.924, R=0.924, RMSE=0.191, Spearman=0.836, train_error=0.0705, train_loss=0.0366]\n",
      "Eval: 7: 100%|██████████| 32/32 [00:08<00:00,  3.77it/s, Pearson=0.779, R=0.779, RMSE=0.322, Spearman=0.758, eval_error=0.138, eval_loss=0.102] \n",
      "Train: 8/30: 100%|██████████| 250/250 [01:42<00:00,  2.45it/s, Pearson=0.933, R=0.933, RMSE=0.18, Spearman=0.84, train_error=0.0644, train_loss=0.0325]  \n",
      "Eval: 8: 100%|██████████| 32/32 [00:08<00:00,  3.76it/s, Pearson=0.775, R=0.775, RMSE=0.329, Spearman=0.766, eval_error=0.139, eval_loss=0.107] \n",
      "Train: 9/30: 100%|██████████| 250/250 [01:42<00:00,  2.45it/s, Pearson=0.943, R=0.943, RMSE=0.167, Spearman=0.844, train_error=0.0534, train_loss=0.0278]\n",
      "Eval: 9: 100%|██████████| 32/32 [00:08<00:00,  3.74it/s, Pearson=0.773, R=0.773, RMSE=0.327, Spearman=0.767, eval_error=0.139, eval_loss=0.106] \n",
      "Train: 10/30: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s, Pearson=0.942, R=0.942, RMSE=0.168, Spearman=0.843, train_error=0.0542, train_loss=0.0281]\n",
      "Eval: 10: 100%|██████████| 32/32 [00:08<00:00,  3.76it/s, Pearson=0.752, R=0.752, RMSE=0.344, Spearman=0.736, eval_error=0.142, eval_loss=0.117]\n",
      "Train: 11/30: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s, Pearson=0.95, R=0.95, RMSE=0.156, Spearman=0.847, train_error=0.0478, train_loss=0.0245]  \n",
      "Eval: 11: 100%|██████████| 32/32 [00:08<00:00,  3.77it/s, Pearson=0.768, R=0.768, RMSE=0.333, Spearman=0.74, eval_error=0.135, eval_loss=0.109]  \n",
      "Train: 12/30: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s, Pearson=0.954, R=0.954, RMSE=0.15, Spearman=0.848, train_error=0.0442, train_loss=0.0224] \n",
      "Eval: 12: 100%|██████████| 32/32 [00:08<00:00,  3.74it/s, Pearson=0.766, R=0.766, RMSE=0.332, Spearman=0.754, eval_error=0.142, eval_loss=0.109]\n",
      "Train: 13/30: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s, Pearson=0.955, R=0.955, RMSE=0.148, Spearman=0.849, train_error=0.0424, train_loss=0.0219]\n",
      "Eval: 13: 100%|██████████| 32/32 [00:08<00:00,  3.72it/s, Pearson=0.768, R=0.768, RMSE=0.331, Spearman=0.755, eval_error=0.14, eval_loss=0.108]  \n",
      "Train: 14/30: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s, Pearson=0.957, R=0.957, RMSE=0.145, Spearman=0.849, train_error=0.0409, train_loss=0.0212]\n",
      "Eval: 14: 100%|██████████| 32/32 [00:08<00:00,  3.75it/s, Pearson=0.758, R=0.758, RMSE=0.34, Spearman=0.751, eval_error=0.141, eval_loss=0.115] \n",
      "Train: 15/30: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s, Pearson=0.958, R=0.958, RMSE=0.143, Spearman=0.85, train_error=0.0396, train_loss=0.0203] \n",
      "Eval: 15: 100%|██████████| 32/32 [00:08<00:00,  3.78it/s, Pearson=0.775, R=0.775, RMSE=0.329, Spearman=0.759, eval_error=0.128, eval_loss=0.107] \n",
      "Train: 16/30: 100%|██████████| 250/250 [01:42<00:00,  2.43it/s, Pearson=0.964, R=0.964, RMSE=0.134, Spearman=0.85, train_error=0.0345, train_loss=0.0179] \n",
      "Eval: 16: 100%|██████████| 32/32 [00:08<00:00,  3.72it/s, Pearson=0.769, R=0.769, RMSE=0.332, Spearman=0.755, eval_error=0.136, eval_loss=0.109]\n",
      "Train: 17/30: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s, Pearson=0.959, R=0.959, RMSE=0.141, Spearman=0.85, train_error=0.0388, train_loss=0.0199] \n",
      "Eval: 17: 100%|██████████| 32/32 [00:08<00:00,  3.73it/s, Pearson=0.766, R=0.766, RMSE=0.334, Spearman=0.746, eval_error=0.135, eval_loss=0.111]\n",
      "Train: 18/30: 100%|██████████| 250/250 [01:42<00:00,  2.43it/s, Pearson=0.964, R=0.964, RMSE=0.133, Spearman=0.852, train_error=0.0344, train_loss=0.0177]\n",
      "Eval: 18: 100%|██████████| 32/32 [00:08<00:00,  3.77it/s, Pearson=0.766, R=0.766, RMSE=0.336, Spearman=0.759, eval_error=0.133, eval_loss=0.111]\n",
      "Train: 19/30: 100%|██████████| 250/250 [01:42<00:00,  2.45it/s, Pearson=0.964, R=0.964, RMSE=0.133, Spearman=0.853, train_error=0.0337, train_loss=0.0177]\n",
      "Eval: 19: 100%|██████████| 32/32 [00:08<00:00,  3.74it/s, Pearson=0.765, R=0.765, RMSE=0.335, Spearman=0.755, eval_error=0.138, eval_loss=0.111]\n",
      "Train: 20/30: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s, Pearson=0.968, R=0.968, RMSE=0.125, Spearman=0.854, train_error=0.0315, train_loss=0.0157]\n",
      "Eval: 20: 100%|██████████| 32/32 [00:08<00:00,  3.73it/s, Pearson=0.765, R=0.765, RMSE=0.338, Spearman=0.748, eval_error=0.13, eval_loss=0.112] \n",
      "Train: 21/30: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s, Pearson=0.972, R=0.972, RMSE=0.118, Spearman=0.855, train_error=0.0271, train_loss=0.0138]\n",
      "Eval: 21: 100%|██████████| 32/32 [00:08<00:00,  3.74it/s, Pearson=0.753, R=0.753, RMSE=0.349, Spearman=0.757, eval_error=0.136, eval_loss=0.12] \n",
      "Train: 22/30: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s, Pearson=0.97, R=0.97, RMSE=0.122, Spearman=0.853, train_error=0.0283, train_loss=0.0149]  \n",
      "Eval: 22: 100%|██████████| 32/32 [00:08<00:00,  3.77it/s, Pearson=0.759, R=0.759, RMSE=0.343, Spearman=0.757, eval_error=0.138, eval_loss=0.117]\n",
      "Train: 23/30: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s, Pearson=0.971, R=0.971, RMSE=0.12, Spearman=0.855, train_error=0.0284, train_loss=0.0144]  \n",
      "Eval: 23: 100%|██████████| 32/32 [00:08<00:00,  3.75it/s, Pearson=0.773, R=0.773, RMSE=0.33, Spearman=0.743, eval_error=0.13, eval_loss=0.108]  \n",
      "Train: 24/30: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s, Pearson=0.967, R=0.967, RMSE=0.128, Spearman=0.854, train_error=0.0322, train_loss=0.0164] \n",
      "Eval: 24: 100%|██████████| 32/32 [00:08<00:00,  3.72it/s, Pearson=0.763, R=0.763, RMSE=0.339, Spearman=0.74, eval_error=0.14, eval_loss=0.113]  \n",
      "Train: 25/30: 100%|██████████| 250/250 [01:42<00:00,  2.43it/s, Pearson=0.971, R=0.971, RMSE=0.12, Spearman=0.856, train_error=0.0275, train_loss=0.0145] \n",
      "Eval: 25: 100%|██████████| 32/32 [00:08<00:00,  3.74it/s, Pearson=0.768, R=0.768, RMSE=0.332, Spearman=0.758, eval_error=0.137, eval_loss=0.109]\n",
      "Train: 26/30: 100%|██████████| 250/250 [01:42<00:00,  2.43it/s, Pearson=0.969, R=0.969, RMSE=0.123, Spearman=0.855, train_error=0.0296, train_loss=0.0152]\n",
      "Eval: 26: 100%|██████████| 32/32 [00:08<00:00,  3.77it/s, Pearson=0.766, R=0.766, RMSE=0.336, Spearman=0.76, eval_error=0.133, eval_loss=0.111] \n",
      "Train: 27/30: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s, Pearson=0.973, R=0.973, RMSE=0.116, Spearman=0.855, train_error=0.0266, train_loss=0.0133]\n",
      "Eval: 27: 100%|██████████| 32/32 [00:08<00:00,  3.74it/s, Pearson=0.767, R=0.767, RMSE=0.337, Spearman=0.76, eval_error=0.132, eval_loss=0.112] \n",
      "Train: 28/30: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s, Pearson=0.971, R=0.971, RMSE=0.12, Spearman=0.853, train_error=0.0274, train_loss=0.0144]  \n",
      "Eval: 28: 100%|██████████| 32/32 [00:08<00:00,  3.77it/s, Pearson=0.763, R=0.763, RMSE=0.337, Spearman=0.738, eval_error=0.135, eval_loss=0.112]\n",
      "Train: 29/30: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s, Pearson=0.976, R=0.976, RMSE=0.109, Spearman=0.856, train_error=0.0232, train_loss=0.012] \n",
      "Eval: 29: 100%|██████████| 32/32 [00:08<00:00,  3.75it/s, Pearson=0.763, R=0.763, RMSE=0.338, Spearman=0.749, eval_error=0.132, eval_loss=0.112]\n",
      "Train: 30/30: 100%|██████████| 250/250 [01:42<00:00,  2.43it/s, Pearson=0.973, R=0.973, RMSE=0.115, Spearman=0.855, train_error=0.0251, train_loss=0.0131]\n",
      "Eval: 30: 100%|██████████| 32/32 [00:08<00:00,  3.79it/s, Pearson=0.77, R=0.77, RMSE=0.333, Spearman=0.749, eval_error=0.133, eval_loss=0.109]  \n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTS\", \"CNSTS\", model_name='x', padding_length=150, batch_size=64, batch_size_eval=128, eval_mode='dev', task_name='SAS_X')\n",
    "\n",
    "for i in trainer(fct_loss='MSELoss'):\n",
    "    a = i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pcpower')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e1e097b6c3c5a2a39328ddbc7de6327b7bd71c15618bc750f041eecacee4167"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
