{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `CLS`-交互式模型训练的样例\n",
    "- model: BERT\n",
    "- dataset: CNSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fct_loss: 损失函数类型, 共有三种`MSELoss`, `BCELoss`, `CrossEntropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTS\", \"CNSTS\", model_name='r2bert', padding_length=150, batch_size=64, batch_size_eval=128, eval_mode='dev', task_name='STS_CNSTS_r2bert')\n",
    "\n",
    "for i in trainer(fct_loss='MSELoss', gpu=[0]):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `CLS`-Siamese模型训练的样例\n",
    "- model: ESIM\n",
    "- dataset: CNSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTSX\", \"CNSTSX\", model_name=\"msim\",  model_type=\"siamese\", padding_length=150, batch_size=64, batch_size_eval=128, eval_mode='dev', task_name='CLS_CNSTSX_msim')\n",
    "\n",
    "for i in trainer(gpu=[0]):\n",
    "    a = i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `CLS`-AC模型训练的样例\n",
    "- model: ACBert\n",
    "- dataset: CNSTSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import Trainer\n",
    "from CC.loaders.acSTSLoader import WordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoModel Choose Model: acbert\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing ACBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing ACBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ACBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ACBertModel were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.0.fuse_layernorm.bias', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.embeddings.position_ids', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.word_transform.bias', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.attn_W']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/250 [00:00<?, ?it/s]/home/lpc/miniconda3/envs/pcpower/lib/python3.9/site-packages/transformers/modeling_utils.py:768: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Train: 1/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.781, train_loss=0.675]\n",
      "Eval: 1: 100%|██████████| 32/32 [00:04<00:00,  7.54it/s, eval_acc=0.855, eval_loss=0.541]\n",
      "Train: 2/30: 100%|██████████| 250/250 [00:46<00:00,  5.33it/s, train_acc=0.87, train_loss=0.518] \n",
      "Eval: 2: 100%|██████████| 32/32 [00:04<00:00,  7.55it/s, eval_acc=0.861, eval_loss=0.526]\n",
      "Train: 3/30: 100%|██████████| 250/250 [00:47<00:00,  5.32it/s, train_acc=0.908, train_loss=0.436]\n",
      "Eval: 3: 100%|██████████| 32/32 [00:04<00:00,  7.52it/s, eval_acc=0.859, eval_loss=0.518]\n",
      "Train: 4/30: 100%|██████████| 250/250 [00:46<00:00,  5.32it/s, train_acc=0.929, train_loss=0.377]\n",
      "Eval: 4: 100%|██████████| 32/32 [00:04<00:00,  7.53it/s, eval_acc=0.869, eval_loss=0.517]\n",
      "Train: 5/30: 100%|██████████| 250/250 [00:47<00:00,  5.32it/s, train_acc=0.945, train_loss=0.334]\n",
      "Eval: 5: 100%|██████████| 32/32 [00:04<00:00,  7.55it/s, eval_acc=0.874, eval_loss=0.509]\n",
      "Train: 6/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.955, train_loss=0.306]\n",
      "Eval: 6: 100%|██████████| 32/32 [00:04<00:00,  7.52it/s, eval_acc=0.87, eval_loss=0.565] \n",
      "Train: 7/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.967, train_loss=0.27] \n",
      "Eval: 7: 100%|██████████| 32/32 [00:04<00:00,  7.52it/s, eval_acc=0.877, eval_loss=0.618]\n",
      "Train: 8/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.968, train_loss=0.258]\n",
      "Eval: 8: 100%|██████████| 32/32 [00:04<00:00,  7.53it/s, eval_acc=0.88, eval_loss=0.581] \n",
      "Train: 9/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.972, train_loss=0.244]\n",
      "Eval: 9: 100%|██████████| 32/32 [00:04<00:00,  7.53it/s, eval_acc=0.879, eval_loss=0.572]\n",
      "Train: 10/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.976, train_loss=0.229]\n",
      "Eval: 10: 100%|██████████| 32/32 [00:04<00:00,  7.51it/s, eval_acc=0.875, eval_loss=0.652]\n",
      "Train: 11/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.978, train_loss=0.217]\n",
      "Eval: 11: 100%|██████████| 32/32 [00:04<00:00,  7.51it/s, eval_acc=0.881, eval_loss=0.661]\n",
      "Train: 12/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.981, train_loss=0.204]\n",
      "Eval: 12: 100%|██████████| 32/32 [00:04<00:00,  7.53it/s, eval_acc=0.878, eval_loss=0.704]\n",
      "Train: 13/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.981, train_loss=0.203]\n",
      "Eval: 13: 100%|██████████| 32/32 [00:04<00:00,  7.52it/s, eval_acc=0.873, eval_loss=0.593]\n",
      "Train: 14/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.984, train_loss=0.194]\n",
      "Eval: 14: 100%|██████████| 32/32 [00:04<00:00,  7.54it/s, eval_acc=0.881, eval_loss=0.576]\n",
      "Train: 15/30: 100%|██████████| 250/250 [00:47<00:00,  5.32it/s, train_acc=0.985, train_loss=0.185]\n",
      "Eval: 15: 100%|██████████| 32/32 [00:04<00:00,  7.51it/s, eval_acc=0.863, eval_loss=0.694]\n",
      "Train: 16/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.986, train_loss=0.178]\n",
      "Eval: 16: 100%|██████████| 32/32 [00:04<00:00,  7.52it/s, eval_acc=0.875, eval_loss=0.661]\n",
      "Train: 17/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.987, train_loss=0.175]\n",
      "Eval: 17: 100%|██████████| 32/32 [00:04<00:00,  7.54it/s, eval_acc=0.871, eval_loss=0.74] \n",
      "Train: 18/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.989, train_loss=0.165]\n",
      "Eval: 18: 100%|██████████| 32/32 [00:04<00:00,  7.51it/s, eval_acc=0.881, eval_loss=0.699]\n",
      "Train: 19/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.99, train_loss=0.157] \n",
      "Eval: 19: 100%|██████████| 32/32 [00:04<00:00,  7.30it/s, eval_acc=0.872, eval_loss=0.775]\n",
      "Train: 20/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.99, train_loss=0.154] \n",
      "Eval: 20: 100%|██████████| 32/32 [00:04<00:00,  7.55it/s, eval_acc=0.878, eval_loss=0.704]\n",
      "Train: 21/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.991, train_loss=0.151]\n",
      "Eval: 21: 100%|██████████| 32/32 [00:04<00:00,  7.50it/s, eval_acc=0.878, eval_loss=0.76] \n",
      "Train: 22/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.99, train_loss=0.151] \n",
      "Eval: 22: 100%|██████████| 32/32 [00:04<00:00,  7.52it/s, eval_acc=0.874, eval_loss=0.816]\n",
      "Train: 23/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.994, train_loss=0.14] \n",
      "Eval: 23: 100%|██████████| 32/32 [00:04<00:00,  7.52it/s, eval_acc=0.874, eval_loss=0.717]\n",
      "Train: 24/30: 100%|██████████| 250/250 [00:47<00:00,  5.32it/s, train_acc=0.991, train_loss=0.143]\n",
      "Eval: 24: 100%|██████████| 32/32 [00:04<00:00,  7.52it/s, eval_acc=0.878, eval_loss=0.728]\n",
      "Train: 25/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.99, train_loss=0.144] \n",
      "Eval: 25: 100%|██████████| 32/32 [00:04<00:00,  7.54it/s, eval_acc=0.876, eval_loss=0.706]\n",
      "Train: 26/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.991, train_loss=0.143]\n",
      "Eval: 26: 100%|██████████| 32/32 [00:04<00:00,  7.52it/s, eval_acc=0.868, eval_loss=0.772]\n",
      "Train: 27/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.991, train_loss=0.14] \n",
      "Eval: 27: 100%|██████████| 32/32 [00:04<00:00,  7.52it/s, eval_acc=0.875, eval_loss=0.768]\n",
      "Train: 28/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.992, train_loss=0.134]\n",
      "Eval: 28: 100%|██████████| 32/32 [00:04<00:00,  7.50it/s, eval_acc=0.873, eval_loss=0.783]\n",
      "Train: 29/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.991, train_loss=0.135]\n",
      "Eval: 29: 100%|██████████| 32/32 [00:04<00:00,  7.52it/s, eval_acc=0.877, eval_loss=0.657]\n",
      "Train: 30/30: 100%|██████████| 250/250 [00:47<00:00,  5.31it/s, train_acc=0.993, train_loss=0.132]\n",
      "Eval: 30: 100%|██████████| 32/32 [00:04<00:00,  7.51it/s, eval_acc=0.878, eval_loss=0.628]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordTokenizer(vocab_file='./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"ACSTS\", \"CNSTSAC\", model_name=\"acbert\", model_type=\"siamese\",  padding_length=150, batch_size=64, batch_size_eval=128, eval_mode='dev', task_name='CLS_CNSTSAC_acbert_simcse_entity')\n",
    "\n",
    "for i in trainer(gpu=[0]):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `CLS`-交互式模型预测的样例\n",
    "- model: BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.predictor import Predictor\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "predictor = Predictor(tokenizer, model_name=\"bert\",  padding_length=256, resume_path='./save_model/Sim/bert/bert_7500.pth', batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in predictor([['你好', '我很好']]):\n",
    "    a = i\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `SAS训练`交互式模型的样例\n",
    "- model: BERT\n",
    "- dataset: CNSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.sas_trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTSX\", \"CNSTSX\", model_name='bert', padding_length=150, batch_size=16, batch_size_eval=512, eval_mode='dev', task_name='SAS_CNSTSX_bert')\n",
    "\n",
    "for i in trainer(fct_loss='MSELoss'):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `SAS训练`Siamese模型的样例\n",
    "- model: SBERT\n",
    "- dataset: CNSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.sas_trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTSX\", \"CNSTSX\", model_name='bimpm', model_type='siamese', padding_length=100, batch_size=32, batch_size_eval=512, eval_mode='dev', task_name='SAS_CNSTSX_bimpm')\n",
    "\n",
    "for i in trainer(fct_loss='MSELoss', lr=1e-3):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `SAS训练`MSIM模型的样例\n",
    "- model: MSIM\n",
    "- dataset: CNSTSX\n",
    "\n",
    "最佳参数:\n",
    "- SAS: A + B + 0.2(C + D)\n",
    "- ASAG: A + B + 0.2(C + D)\n",
    "- SFR: 0.1A + B + 0.2(C + D)\n",
    "- CNSTS: A + B + 0.2(C + D)\n",
    "\n",
    "FewShot最佳参数:\n",
    "- SAS: A + B + 0.2(C + D)\n",
    "- ASAG: 0.1A + 0.8B + 0.2(C + D)\n",
    "\n",
    "Fewshot Prompt Fine-tuning时 ASAG数据集需要1e-4学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.sas_trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTSX\", \"SFR\", model_name='msim', model_type='siamese', padding_length=256, batch_size=8, batch_size_eval=256, eval_mode='dev', task_name='SAS_SFR_msim')\n",
    "\n",
    "for i in trainer(fct_loss='MSELoss'):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `SIMCSE` 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.cl_trainer import CLTrainer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = CLTrainer(tokenizer, \"SIMCSE_STS\", \"SAS\", model_name='simcse', model_type='siamese', padding_length=256, batch_size=16, batch_size_eval=256, eval_mode='dev', task_name='SIMCSE_SAS')\n",
    "\n",
    "for i in trainer(fct_loss='MSELoss'):\n",
    "    a = i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.prompt_trainer import PromptTrainer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = PromptTrainer(tokenizer, \"SASPrompt\", \"ASAGPrompt\", model_name='bertlm', padding_length=256, batch_size=16, batch_size_eval=256, eval_mode='dev', task_name='Prompt_ASAG')\n",
    "\n",
    "for i in trainer():\n",
    "    a = i\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pcpower')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e1e097b6c3c5a2a39328ddbc7de6327b7bd71c15618bc750f041eecacee4167"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
