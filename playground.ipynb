{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 交互式模型训练的样例\n",
    "- model: BERT\n",
    "- dataset: CNSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fct_loss: 损失函数类型, 共有三种`MSELoss`, `BCELoss`, `CrossEntropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTS\", \"CNSTS\", padding_length=150, batch_size=64, batch_size_eval=128, eval_mode='dev')\n",
    "\n",
    "for i in trainer(fct_loss='BCELoss'):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Siamese模型训练的样例\n",
    "- model: ESIM\n",
    "- dataset: CNSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTS\", \"CNSTS\", model_name=\"esim\",  model_type=\"siamese\", padding_length=150, batch_size=64, batch_size_eval=128, eval_mode='dev', task_name='ESIM_Sim')\n",
    "\n",
    "for i in trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 交互式模型SAS训练的样例\n",
    "- model: BERT\n",
    "- dataset: CNSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.sas_trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpc/miniconda3/envs/pcpower/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1642: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "100%|██████████| 409/409 [00:00<00:00, 16245.14it/s]\n",
      "100%|██████████| 176/176 [00:00<00:00, 16146.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoModel Choose Model: bert\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]/home/lpc/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/lpc/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:117: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Train: 1/30: 100%|██████████| 26/26 [00:15<00:00,  1.72it/s, Pearson=-.116, R=-.116, RMSE=0.374, Spearman=-.108, train_error=0.317, train_loss=0.141]\n",
      "Eval: 1: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s, Pearson=0.386, R=0.386, RMSE=0.326, Spearman=0.351, eval_error=0.272, eval_loss=0.107]\n",
      "Train: 2/30: 100%|██████████| 26/26 [00:08<00:00,  3.15it/s, Pearson=0.409, R=0.409, RMSE=0.321, Spearman=0.379, train_error=0.271, train_loss=0.104] \n",
      "Eval: 2: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s, Pearson=0.692, R=0.692, RMSE=0.273, Spearman=0.696, eval_error=0.226, eval_loss=0.0745]\n",
      "Train: 3/30: 100%|██████████| 26/26 [00:08<00:00,  3.11it/s, Pearson=0.775, R=0.775, RMSE=0.224, Spearman=0.764, train_error=0.183, train_loss=0.0498]\n",
      "Eval: 3: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, Pearson=0.819, R=0.819, RMSE=0.194, Spearman=0.852, eval_error=0.157, eval_loss=0.0377]\n",
      "Train: 4/30: 100%|██████████| 26/26 [00:08<00:00,  3.06it/s, Pearson=0.866, R=0.866, RMSE=0.175, Spearman=0.859, train_error=0.147, train_loss=0.0305]\n",
      "Eval: 4: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, Pearson=0.808, R=0.808, RMSE=0.246, Spearman=0.842, eval_error=0.191, eval_loss=0.0608]\n",
      "Train: 5/30: 100%|██████████| 26/26 [00:08<00:00,  3.02it/s, Pearson=0.907, R=0.907, RMSE=0.147, Spearman=0.906, train_error=0.123, train_loss=0.0213]\n",
      "Eval: 5: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s, Pearson=0.855, R=0.855, RMSE=0.184, Spearman=0.878, eval_error=0.143, eval_loss=0.0338]\n",
      "Train: 6/30: 100%|██████████| 26/26 [00:08<00:00,  3.12it/s, Pearson=0.941, R=0.941, RMSE=0.118, Spearman=0.934, train_error=0.0941, train_loss=0.0138]\n",
      "Eval: 6: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s, Pearson=0.868, R=0.868, RMSE=0.178, Spearman=0.891, eval_error=0.124, eval_loss=0.0316]\n",
      "Train: 7/30: 100%|██████████| 26/26 [00:08<00:00,  3.15it/s, Pearson=0.939, R=0.939, RMSE=0.12, Spearman=0.935, train_error=0.0918, train_loss=0.0148]  \n",
      "Eval: 7: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, Pearson=0.813, R=0.813, RMSE=0.239, Spearman=0.849, eval_error=0.171, eval_loss=0.057]\n",
      "Train: 8/30: 100%|██████████| 26/26 [00:08<00:00,  3.06it/s, Pearson=0.946, R=0.946, RMSE=0.114, Spearman=0.941, train_error=0.0856, train_loss=0.0128]\n",
      "Eval: 8: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s, Pearson=0.805, R=0.805, RMSE=0.252, Spearman=0.856, eval_error=0.177, eval_loss=0.0637]\n",
      "Train: 9/30: 100%|██████████| 26/26 [00:08<00:00,  3.04it/s, Pearson=0.92, R=0.92, RMSE=0.138, Spearman=0.909, train_error=0.0982, train_loss=0.0187]   \n",
      "Eval: 9: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s, Pearson=0.849, R=0.849, RMSE=0.179, Spearman=0.859, eval_error=0.125, eval_loss=0.0319]\n",
      "Train: 10/30: 100%|██████████| 26/26 [00:08<00:00,  3.15it/s, Pearson=0.906, R=0.906, RMSE=0.149, Spearman=0.902, train_error=0.107, train_loss=0.0219]\n",
      "Eval: 10: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s, Pearson=0.84, R=0.84, RMSE=0.184, Spearman=0.858, eval_error=0.125, eval_loss=0.0339]\n",
      "Train: 11/30: 100%|██████████| 26/26 [00:08<00:00,  3.03it/s, Pearson=0.954, R=0.954, RMSE=0.105, Spearman=0.948, train_error=0.0743, train_loss=0.0108]  \n",
      "Eval: 11: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, Pearson=0.882, R=0.882, RMSE=0.173, Spearman=0.893, eval_error=0.116, eval_loss=0.0299]\n",
      "Train: 12/30: 100%|██████████| 26/26 [00:08<00:00,  3.09it/s, Pearson=0.954, R=0.954, RMSE=0.106, Spearman=0.946, train_error=0.0751, train_loss=0.0116]\n",
      "Eval: 12: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, Pearson=0.876, R=0.876, RMSE=0.17, Spearman=0.888, eval_error=0.108, eval_loss=0.0291]\n",
      "Train: 13/30: 100%|██████████| 26/26 [00:08<00:00,  3.10it/s, Pearson=0.966, R=0.966, RMSE=0.0911, Spearman=0.958, train_error=0.0648, train_loss=0.00829]\n",
      "Eval: 13: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s, Pearson=0.87, R=0.87, RMSE=0.167, Spearman=0.882, eval_error=0.112, eval_loss=0.0279]\n",
      "Train: 14/30: 100%|██████████| 26/26 [00:08<00:00,  3.14it/s, Pearson=0.979, R=0.979, RMSE=0.0713, Spearman=0.973, train_error=0.0498, train_loss=0.00504]\n",
      "Eval: 14: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, Pearson=0.887, R=0.887, RMSE=0.157, Spearman=0.894, eval_error=0.103, eval_loss=0.0245]\n",
      "Train: 15/30: 100%|██████████| 26/26 [00:08<00:00,  3.00it/s, Pearson=0.976, R=0.976, RMSE=0.0765, Spearman=0.969, train_error=0.0533, train_loss=0.0058] \n",
      "Eval: 15: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s, Pearson=0.877, R=0.877, RMSE=0.165, Spearman=0.89, eval_error=0.1, eval_loss=0.0272]\n",
      "Train: 16/30: 100%|██████████| 26/26 [00:08<00:00,  3.07it/s, Pearson=0.978, R=0.978, RMSE=0.0735, Spearman=0.971, train_error=0.0503, train_loss=0.00533]\n",
      "Eval: 16: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s, Pearson=0.9, R=0.9, RMSE=0.154, Spearman=0.911, eval_error=0.105, eval_loss=0.0237]\n",
      "Train: 17/30: 100%|██████████| 26/26 [00:08<00:00,  3.14it/s, Pearson=0.98, R=0.98, RMSE=0.0693, Spearman=0.973, train_error=0.0463, train_loss=0.00473]  \n",
      "Eval: 17: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s, Pearson=0.889, R=0.889, RMSE=0.157, Spearman=0.901, eval_error=0.0961, eval_loss=0.0245]\n",
      "Train: 18/30: 100%|██████████| 26/26 [00:08<00:00,  3.09it/s, Pearson=0.985, R=0.985, RMSE=0.0601, Spearman=0.975, train_error=0.0396, train_loss=0.00356]\n",
      "Eval: 18: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, Pearson=0.885, R=0.885, RMSE=0.16, Spearman=0.897, eval_error=0.103, eval_loss=0.0257]\n",
      "Train: 19/30: 100%|██████████| 26/26 [00:08<00:00,  2.92it/s, Pearson=0.983, R=0.983, RMSE=0.0637, Spearman=0.975, train_error=0.0431, train_loss=0.004]  \n",
      "Eval: 19: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s, Pearson=0.884, R=0.884, RMSE=0.161, Spearman=0.901, eval_error=0.0938, eval_loss=0.0259]\n",
      "Train: 20/30: 100%|██████████| 26/26 [00:08<00:00,  3.09it/s, Pearson=0.984, R=0.984, RMSE=0.0625, Spearman=0.974, train_error=0.0399, train_loss=0.00385]\n",
      "Eval: 20: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s, Pearson=0.89, R=0.89, RMSE=0.154, Spearman=0.901, eval_error=0.0933, eval_loss=0.0239]\n",
      "Train: 21/30: 100%|██████████| 26/26 [00:08<00:00,  3.02it/s, Pearson=0.988, R=0.988, RMSE=0.0546, Spearman=0.977, train_error=0.0367, train_loss=0.00293]\n",
      "Eval: 21: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, Pearson=0.892, R=0.892, RMSE=0.154, Spearman=0.902, eval_error=0.0943, eval_loss=0.0238]\n",
      "Train: 22/30: 100%|██████████| 26/26 [00:08<00:00,  3.01it/s, Pearson=0.986, R=0.986, RMSE=0.0583, Spearman=0.976, train_error=0.0388, train_loss=0.00335]\n",
      "Eval: 22: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s, Pearson=0.896, R=0.896, RMSE=0.15, Spearman=0.905, eval_error=0.093, eval_loss=0.0226]\n",
      "Train: 23/30: 100%|██████████| 26/26 [00:08<00:00,  3.16it/s, Pearson=0.988, R=0.988, RMSE=0.0535, Spearman=0.978, train_error=0.0354, train_loss=0.00283]\n",
      "Eval: 23: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s, Pearson=0.884, R=0.884, RMSE=0.166, Spearman=0.898, eval_error=0.103, eval_loss=0.0277]\n",
      "Train: 24/30: 100%|██████████| 26/26 [00:08<00:00,  3.05it/s, Pearson=0.986, R=0.986, RMSE=0.0577, Spearman=0.976, train_error=0.0376, train_loss=0.0033] \n",
      "Eval: 24: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, Pearson=0.884, R=0.884, RMSE=0.166, Spearman=0.898, eval_error=0.105, eval_loss=0.0274]\n",
      "Train: 25/30: 100%|██████████| 26/26 [00:08<00:00,  3.04it/s, Pearson=0.985, R=0.985, RMSE=0.0603, Spearman=0.977, train_error=0.0384, train_loss=0.00359]\n",
      "Eval: 25: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, Pearson=0.881, R=0.881, RMSE=0.173, Spearman=0.899, eval_error=0.109, eval_loss=0.0299]\n",
      "Train: 26/30: 100%|██████████| 26/26 [00:08<00:00,  3.12it/s, Pearson=0.985, R=0.985, RMSE=0.061, Spearman=0.975, train_error=0.0389, train_loss=0.00366] \n",
      "Eval: 26: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s, Pearson=0.871, R=0.871, RMSE=0.188, Spearman=0.89, eval_error=0.126, eval_loss=0.0353]\n",
      "Train: 27/30: 100%|██████████| 26/26 [00:08<00:00,  3.17it/s, Pearson=0.988, R=0.988, RMSE=0.0553, Spearman=0.979, train_error=0.0366, train_loss=0.00303]\n",
      "Eval: 27: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, Pearson=0.859, R=0.859, RMSE=0.204, Spearman=0.887, eval_error=0.139, eval_loss=0.0415]\n",
      "Train: 28/30: 100%|██████████| 26/26 [00:08<00:00,  3.08it/s, Pearson=0.979, R=0.979, RMSE=0.0721, Spearman=0.967, train_error=0.0459, train_loss=0.00526]\n",
      "Eval: 28: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s, Pearson=0.855, R=0.855, RMSE=0.213, Spearman=0.884, eval_error=0.149, eval_loss=0.0456]\n",
      "Train: 29/30: 100%|██████████| 26/26 [00:08<00:00,  3.02it/s, Pearson=0.969, R=0.969, RMSE=0.0869, Spearman=0.962, train_error=0.0524, train_loss=0.00744]\n",
      "Eval: 29: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s, Pearson=0.854, R=0.854, RMSE=0.202, Spearman=0.871, eval_error=0.134, eval_loss=0.0409]\n",
      "Train: 30/30: 100%|██████████| 26/26 [00:08<00:00,  3.14it/s, Pearson=0.977, R=0.977, RMSE=0.0746, Spearman=0.969, train_error=0.051, train_loss=0.0055]  \n",
      "Eval: 30: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, Pearson=0.847, R=0.847, RMSE=0.212, Spearman=0.871, eval_error=0.146, eval_loss=0.0449]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTSX\", \"SAS\", model_name='bert', padding_length=256, batch_size=16, batch_size_eval=512, eval_mode='dev', task_name='SAS_SAS_BERT')\n",
    "\n",
    "for i in trainer(fct_loss='MSELoss'):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 交互式模型SAS训练的样例\n",
    "- model: MSIM\n",
    "- dataset: CNSTSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.sas_trainer import Trainer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpc/miniconda3/envs/pcpower/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1642: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "100%|██████████| 409/409 [00:00<00:00, 16258.84it/s]\n",
      "100%|██████████| 176/176 [00:00<00:00, 16208.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoModel Choose Model: msim\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]/home/lpc/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/lpc/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:117: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Train: 1/30: 100%|██████████| 26/26 [00:17<00:00,  1.52it/s, Pearson=-.0528, R=-.0528, RMSE=1.47, Spearman=0.033, train_error=1.12, train_loss=2.14]  \n",
      "Eval: 1: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s, Pearson=0.395, R=0.395, RMSE=0.391, Spearman=0.391, eval_error=0.321, eval_loss=0.153]\n",
      "Train: 2/30: 100%|██████████| 26/26 [00:09<00:00,  2.70it/s, Pearson=0.499, R=0.499, RMSE=0.306, Spearman=0.485, train_error=0.247, train_loss=0.0922]\n",
      "Eval: 2: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, Pearson=0.8, R=0.8, RMSE=0.217, Spearman=0.818, eval_error=0.176, eval_loss=0.0471]\n",
      "Train: 3/30: 100%|██████████| 26/26 [00:10<00:00,  2.54it/s, Pearson=0.812, R=0.812, RMSE=0.204, Spearman=0.793, train_error=0.162, train_loss=0.0411]\n",
      "Eval: 3: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s, Pearson=0.814, R=0.814, RMSE=0.207, Spearman=0.85, eval_error=0.162, eval_loss=0.0429]\n",
      "Train: 4/30: 100%|██████████| 26/26 [00:09<00:00,  2.69it/s, Pearson=0.889, R=0.889, RMSE=0.16, Spearman=0.891, train_error=0.126, train_loss=0.0255] \n",
      "Eval: 4: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, Pearson=0.832, R=0.832, RMSE=0.201, Spearman=0.861, eval_error=0.155, eval_loss=0.0404]\n",
      "Train: 5/30: 100%|██████████| 26/26 [00:10<00:00,  2.55it/s, Pearson=0.928, R=0.928, RMSE=0.13, Spearman=0.927, train_error=0.105, train_loss=0.0168] \n",
      "Eval: 5: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, Pearson=0.822, R=0.822, RMSE=0.196, Spearman=0.845, eval_error=0.147, eval_loss=0.0384]\n",
      "Train: 6/30: 100%|██████████| 26/26 [00:09<00:00,  2.70it/s, Pearson=0.929, R=0.929, RMSE=0.13, Spearman=0.925, train_error=0.104, train_loss=0.0171]   \n",
      "Eval: 6: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, Pearson=0.839, R=0.839, RMSE=0.235, Spearman=0.878, eval_error=0.184, eval_loss=0.0552]\n",
      "Train: 7/30: 100%|██████████| 26/26 [00:10<00:00,  2.56it/s, Pearson=0.936, R=0.936, RMSE=0.123, Spearman=0.94, train_error=0.102, train_loss=0.015]  \n",
      "Eval: 7: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s, Pearson=0.843, R=0.843, RMSE=0.208, Spearman=0.875, eval_error=0.157, eval_loss=0.0435]\n",
      "Train: 8/30: 100%|██████████| 26/26 [00:09<00:00,  2.63it/s, Pearson=0.948, R=0.948, RMSE=0.113, Spearman=0.945, train_error=0.0917, train_loss=0.0126]  \n",
      "Eval: 8: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s, Pearson=0.84, R=0.84, RMSE=0.226, Spearman=0.872, eval_error=0.171, eval_loss=0.0509]\n",
      "Train: 9/30: 100%|██████████| 26/26 [00:09<00:00,  2.62it/s, Pearson=0.936, R=0.936, RMSE=0.124, Spearman=0.94, train_error=0.0977, train_loss=0.0152]\n",
      "Eval: 9: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s, Pearson=0.859, R=0.859, RMSE=0.174, Spearman=0.875, eval_error=0.127, eval_loss=0.0303]\n",
      "Train: 10/30: 100%|██████████| 26/26 [00:09<00:00,  2.61it/s, Pearson=0.945, R=0.945, RMSE=0.114, Spearman=0.94, train_error=0.0914, train_loss=0.0129]   \n",
      "Eval: 10: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s, Pearson=0.87, R=0.87, RMSE=0.169, Spearman=0.882, eval_error=0.127, eval_loss=0.0285]\n",
      "Train: 11/30: 100%|██████████| 26/26 [00:09<00:00,  2.67it/s, Pearson=0.956, R=0.956, RMSE=0.104, Spearman=0.957, train_error=0.0831, train_loss=0.0107]  \n",
      "Eval: 11: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s, Pearson=0.867, R=0.867, RMSE=0.173, Spearman=0.876, eval_error=0.133, eval_loss=0.03]\n",
      "Train: 12/30: 100%|██████████| 26/26 [00:09<00:00,  2.61it/s, Pearson=0.94, R=0.94, RMSE=0.12, Spearman=0.94, train_error=0.0998, train_loss=0.0146]     \n",
      "Eval: 12: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s, Pearson=0.847, R=0.847, RMSE=0.224, Spearman=0.894, eval_error=0.173, eval_loss=0.0501]\n",
      "Train: 13/30: 100%|██████████| 26/26 [00:09<00:00,  2.70it/s, Pearson=0.971, R=0.971, RMSE=0.0844, Spearman=0.967, train_error=0.0669, train_loss=0.00705]\n",
      "Eval: 13: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, Pearson=0.845, R=0.845, RMSE=0.218, Spearman=0.885, eval_error=0.155, eval_loss=0.0474]\n",
      "Train: 14/30: 100%|██████████| 26/26 [00:10<00:00,  2.56it/s, Pearson=0.978, R=0.978, RMSE=0.0735, Spearman=0.974, train_error=0.0575, train_loss=0.00532]\n",
      "Eval: 14: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s, Pearson=0.872, R=0.872, RMSE=0.177, Spearman=0.888, eval_error=0.12, eval_loss=0.0315]\n",
      "Train: 15/30: 100%|██████████| 26/26 [00:09<00:00,  2.74it/s, Pearson=0.985, R=0.985, RMSE=0.0607, Spearman=0.98, train_error=0.0494, train_loss=0.00365] \n",
      "Eval: 15: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, Pearson=0.87, R=0.87, RMSE=0.184, Spearman=0.89, eval_error=0.128, eval_loss=0.034]\n",
      "Train: 16/30: 100%|██████████| 26/26 [00:10<00:00,  2.59it/s, Pearson=0.987, R=0.987, RMSE=0.0568, Spearman=0.981, train_error=0.0467, train_loss=0.00323]\n",
      "Eval: 16: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s, Pearson=0.861, R=0.861, RMSE=0.195, Spearman=0.895, eval_error=0.14, eval_loss=0.038]\n",
      "Train: 17/30: 100%|██████████| 26/26 [00:09<00:00,  2.71it/s, Pearson=0.989, R=0.989, RMSE=0.053, Spearman=0.982, train_error=0.0429, train_loss=0.00278] \n",
      "Eval: 17: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, Pearson=0.867, R=0.867, RMSE=0.189, Spearman=0.893, eval_error=0.13, eval_loss=0.0355]\n",
      "Train: 18/30: 100%|██████████| 26/26 [00:09<00:00,  2.60it/s, Pearson=0.988, R=0.988, RMSE=0.054, Spearman=0.98, train_error=0.043, train_loss=0.0029]    \n",
      "Eval: 18: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, Pearson=0.874, R=0.874, RMSE=0.179, Spearman=0.893, eval_error=0.121, eval_loss=0.032]\n",
      "Train: 19/30: 100%|██████████| 26/26 [00:09<00:00,  2.71it/s, Pearson=0.988, R=0.988, RMSE=0.0551, Spearman=0.98, train_error=0.0429, train_loss=0.003]   \n",
      "Eval: 19: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, Pearson=0.868, R=0.868, RMSE=0.182, Spearman=0.885, eval_error=0.116, eval_loss=0.0329]\n",
      "Train: 20/30: 100%|██████████| 26/26 [00:10<00:00,  2.55it/s, Pearson=0.987, R=0.987, RMSE=0.057, Spearman=0.979, train_error=0.0445, train_loss=0.00324] \n",
      "Eval: 20: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, Pearson=0.877, R=0.877, RMSE=0.163, Spearman=0.886, eval_error=0.11, eval_loss=0.0264]\n",
      "Train: 21/30: 100%|██████████| 26/26 [00:09<00:00,  2.72it/s, Pearson=0.984, R=0.984, RMSE=0.0619, Spearman=0.978, train_error=0.0494, train_loss=0.00382]\n",
      "Eval: 21: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, Pearson=0.871, R=0.871, RMSE=0.17, Spearman=0.877, eval_error=0.122, eval_loss=0.029]\n",
      "Train: 22/30: 100%|██████████| 26/26 [00:10<00:00,  2.56it/s, Pearson=0.973, R=0.973, RMSE=0.08, Spearman=0.966, train_error=0.0626, train_loss=0.00639]  \n",
      "Eval: 22: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, Pearson=0.876, R=0.876, RMSE=0.179, Spearman=0.875, eval_error=0.132, eval_loss=0.0321]\n",
      "Train: 23/30: 100%|██████████| 26/26 [00:09<00:00,  2.73it/s, Pearson=0.98, R=0.98, RMSE=0.0691, Spearman=0.974, train_error=0.0544, train_loss=0.00475]  \n",
      "Eval: 23: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, Pearson=0.865, R=0.865, RMSE=0.203, Spearman=0.871, eval_error=0.155, eval_loss=0.0411]\n",
      "Train: 24/30: 100%|██████████| 26/26 [00:10<00:00,  2.54it/s, Pearson=0.955, R=0.955, RMSE=0.103, Spearman=0.95, train_error=0.0805, train_loss=0.0105] \n",
      "Eval: 24: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, Pearson=0.885, R=0.885, RMSE=0.174, Spearman=0.887, eval_error=0.125, eval_loss=0.0303]\n",
      "Train: 25/30: 100%|██████████| 26/26 [00:09<00:00,  2.75it/s, Pearson=0.975, R=0.975, RMSE=0.077, Spearman=0.971, train_error=0.0584, train_loss=0.00593] \n",
      "Eval: 25: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s, Pearson=0.879, R=0.879, RMSE=0.186, Spearman=0.886, eval_error=0.141, eval_loss=0.0347]\n",
      "Train: 26/30: 100%|██████████| 26/26 [00:10<00:00,  2.52it/s, Pearson=0.967, R=0.967, RMSE=0.0893, Spearman=0.963, train_error=0.0653, train_loss=0.00791]\n",
      "Eval: 26: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, Pearson=0.824, R=0.824, RMSE=0.206, Spearman=0.824, eval_error=0.158, eval_loss=0.0424]\n",
      "Train: 27/30: 100%|██████████| 26/26 [00:09<00:00,  2.69it/s, Pearson=0.975, R=0.975, RMSE=0.0776, Spearman=0.972, train_error=0.0611, train_loss=0.00601]\n",
      "Eval: 27: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, Pearson=0.85, R=0.85, RMSE=0.184, Spearman=0.857, eval_error=0.138, eval_loss=0.034]\n",
      "Train: 28/30: 100%|██████████| 26/26 [00:10<00:00,  2.58it/s, Pearson=0.973, R=0.973, RMSE=0.0802, Spearman=0.966, train_error=0.0621, train_loss=0.00639]\n",
      "Eval: 28: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, Pearson=0.869, R=0.869, RMSE=0.171, Spearman=0.875, eval_error=0.127, eval_loss=0.0293]\n",
      "Train: 29/30: 100%|██████████| 26/26 [00:09<00:00,  2.69it/s, Pearson=0.975, R=0.975, RMSE=0.0781, Spearman=0.968, train_error=0.0617, train_loss=0.00602]\n",
      "Eval: 29: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, Pearson=0.88, R=0.88, RMSE=0.171, Spearman=0.884, eval_error=0.127, eval_loss=0.0293]\n",
      "Train: 30/30: 100%|██████████| 26/26 [00:10<00:00,  2.60it/s, Pearson=0.977, R=0.977, RMSE=0.0745, Spearman=0.972, train_error=0.0562, train_loss=0.00548]\n",
      "Eval: 30: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, Pearson=0.853, R=0.853, RMSE=0.177, Spearman=0.859, eval_error=0.125, eval_loss=0.0314]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "trainer = Trainer(tokenizer, \"CNSTSX\", \"SAS\", model_name='msim', model_type='siamese', padding_length=256, batch_size=16, batch_size_eval=256, eval_mode='dev', task_name='SAS_SAS_MSIM')\n",
    "\n",
    "for i in trainer(fct_loss='MSELoss'):\n",
    "    a = i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pcpower')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e1e097b6c3c5a2a39328ddbc7de6327b7bd71c15618bc750f041eecacee4167"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
