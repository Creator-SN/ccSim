{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析句子长度对模型性能的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def RMSE(x, y):\n",
    "    return (x - y) ** 2\n",
    "\n",
    "\n",
    "with open('./dataset/CHIP-STS/dev.json') as f:\n",
    "    ori_list = f.read()\n",
    "ori_list = json.loads(ori_list)\n",
    "\n",
    "with open('./data_record/SAS_CNSTSX_r2bert/predict_gold.csv') as f:\n",
    "    pred_gold_list = f.read()\n",
    "    pred_gold_list = pred_gold_list.split('\\n')\n",
    "    if pred_gold_list[-1] == '':\n",
    "        pred_gold_list = pred_gold_list[:-1]\n",
    "\n",
    "slot = {\n",
    "    # 10: {\n",
    "    #     'RMSE': [],\n",
    "    #     'count': 0\n",
    "    # },\n",
    "    25: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    },\n",
    "    50: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    },\n",
    "    75: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    },\n",
    "    100: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    },\n",
    "    125: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    },\n",
    "    150: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    },\n",
    "    175: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    },\n",
    "    200: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    },\n",
    "    999: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "for idx, item in enumerate(ori_list):\n",
    "    ans = item['text2']\n",
    "    gold, pred = pred_gold_list[idx].split('\\t')\n",
    "    length = len(ans)\n",
    "    for key in slot:\n",
    "        if length > key:\n",
    "            continue\n",
    "        else:\n",
    "            slot[key]['count'] += 1\n",
    "            slot[key]['RMSE'].append(RMSE(float(pred), float(gold)))\n",
    "            break\n",
    "\n",
    "for key in slot:\n",
    "    print('<={}: count: {}, rmse: {:.3f}'.format(key, slot[key]['count'], math.sqrt(np.mean(slot[key]['RMSE']))))\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析不同评分对模型性能的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def RMSE(x, y):\n",
    "    return (x - y) ** 2\n",
    "\n",
    "def print_model_rmse_in_gold_dis(model_list=[], dataset_name='SAS'):\n",
    "    for model in model_list:\n",
    "        with open('./data_record/SAS_{}_{}/predict_gold.csv'.format(dataset_name, model)) as f:\n",
    "            pred_gold_list = f.read()\n",
    "            pred_gold_list = pred_gold_list.split('\\n')\n",
    "            if pred_gold_list[-1] == '':\n",
    "                pred_gold_list = pred_gold_list[:-1]\n",
    "\n",
    "        slot = {\n",
    "            0.2: {\n",
    "                'RMSE': [],\n",
    "                'count': 0\n",
    "            },\n",
    "            0.4: {\n",
    "                'RMSE': [],\n",
    "                'count': 0\n",
    "            },\n",
    "            0.6: {\n",
    "                'RMSE': [],\n",
    "                'count': 0\n",
    "            },\n",
    "            0.8: {\n",
    "                'RMSE': [],\n",
    "                'count': 0\n",
    "            },\n",
    "            1.2: {\n",
    "                'RMSE': [],\n",
    "                'count': 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for idx, line in enumerate(pred_gold_list):\n",
    "            pred, gold = pred_gold_list[idx].split('\\t')\n",
    "            for key in slot:\n",
    "                if float(gold) > key:\n",
    "                    continue\n",
    "                else:\n",
    "                    slot[key]['count'] += 1\n",
    "                    slot[key]['RMSE'].append(RMSE(float(pred), float(gold)))\n",
    "                    break\n",
    "        \n",
    "        print('{}:'.format(model))\n",
    "        for key in slot:\n",
    "            print('<={}: count: {}, rmse: {:.3f}'.format(key, slot[key]['count'], math.sqrt(np.mean(slot[key]['RMSE']))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_rmse_in_gold_dis(['textcnn', 'bert', 'bimpm', 'esim', 'sbert', 'r2bert'], 'ASAG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_rmse_in_gold_dis(['bert'], 'SFR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算模型分类性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def F1(X, Y):\n",
    "    result_dict = {}\n",
    "    for idx, _ in enumerate(X):\n",
    "        x, y = X[idx], Y[idx]\n",
    "        if x not in result_dict:\n",
    "            result_dict[x] = {\n",
    "                'TP': 0,\n",
    "                'FP': 0,\n",
    "                'FN': 0\n",
    "            }\n",
    "        if y not in result_dict:\n",
    "            result_dict[y] = {\n",
    "                'TP': 0,\n",
    "                'FP': 0,\n",
    "                'FN': 0\n",
    "            }\n",
    "        if x == y:\n",
    "            result_dict[x]['TP'] += 1\n",
    "        else:\n",
    "            result_dict[x]['FP'] += 1\n",
    "            result_dict[y]['FN'] += 1\n",
    "\n",
    "    P = []\n",
    "    R = []\n",
    "    F1Score = []\n",
    "    for key in result_dict:\n",
    "        TP, FP, FN = result_dict[key]['TP'], result_dict[key]['FP'], result_dict[key]['FN']\n",
    "        if TP == 0:\n",
    "            continue\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        P.append(precision)\n",
    "        R.append(recall)\n",
    "        F1Score.append(f1)\n",
    "\n",
    "    return np.mean(P), np.mean(R), np.mean(F1Score)\n",
    "\n",
    "def map(score):\n",
    "    if score <= 3:\n",
    "        return 0\n",
    "    elif score <= 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "\n",
    "def print_model_f1(model_list=[], dataset_name='SAS'):\n",
    "    for model in model_list:\n",
    "        with open('./data_record/SAS_{}_{}/predict_gold{}.csv'.format(dataset_name, model, '_sota' if model == 'msim' else '')) as f:\n",
    "            pred_gold_list = f.read()\n",
    "            pred_gold_list = pred_gold_list.split('\\n')\n",
    "            if pred_gold_list[-1] == '':\n",
    "                pred_gold_list = pred_gold_list[:-1]\n",
    "\n",
    "        pred_list = []\n",
    "        gold_list = []\n",
    "\n",
    "        for line in pred_gold_list:\n",
    "            pred, gold = line.split('\\t')\n",
    "            pred = round(float(pred) * 10)\n",
    "            gold = round(float(gold) * 10)\n",
    "            pred_list.append(map(pred))\n",
    "            gold_list.append(map(gold))\n",
    "\n",
    "        result = F1(pred_list, gold_list)\n",
    "\n",
    "        print('{}\\n P: {:.2f}, R: {:.2f}, F1: {:.2f}\\n'.format(\n",
    "            model, result[0] * 100, result[1] * 100, result[2] * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_f1(['textcnn', 'esim', 'bimpm', 'bert', 'sbert', 'r2bert'], 'CNSTSX')\n",
    "print_model_f1(['msim'], 'CNSTSX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算数据集句向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertConfig, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "config = BertConfig.from_pretrained('./model/chinese_wwm_ext/bert_config.json')\n",
    "model = BertModel.from_pretrained('./model/chinese_wwm_ext/pytorch_model.bin', config=config)\n",
    "\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "with open('./dataset/CHIP-STS/train.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "ref_list = []\n",
    "std_list = []\n",
    "\n",
    "for idx, line in tqdm(enumerate(data)):\n",
    "    ref, std = line['text1'], line['text2']\n",
    "    ref_T = tokenizer(ref, return_tensors=\"pt\")\n",
    "    std_T = tokenizer(std, return_tensors=\"pt\")\n",
    "    ref_T = {k: v.cuda() for k, v in ref_T.items()}\n",
    "    std_T = {k: v.cuda() for k, v in std_T.items()}\n",
    "    output = model(**ref_T)\n",
    "    ref_list.append(output.pooler_output[0].tolist())\n",
    "    output = model(**std_T)\n",
    "    std_list.append(output.pooler_output[0].tolist())\n",
    "\n",
    "if not os.path.exists('./data_record/tSNE/CNSTS'):\n",
    "    os.mkdir('./data_record/tSNE/CNSTS')\n",
    "\n",
    "with open('./data_record/tSNE/CNSTS/ref.tsv', 'w+') as f:\n",
    "    for ref in ref_list:\n",
    "        f.write('\\t'.join([str(i) for i in ref]) + '\\n')\n",
    "\n",
    "with open('./data_record/tSNE/CNSTS/std.tsv', 'w+') as f:\n",
    "    for std in std_list:\n",
    "        f.write('\\t'.join(str(i) for i in std) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STS样例预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from CC.predictor import Predictor\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "predictor = Predictor(tokenizer, model_name=\"bert\",  padding_length=150, resume_path='./save_model/STS_CNSTS_bert/bert/bert_2000.pth', batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/CHIP-STS/dev.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "eval_list = []\n",
    "for line in data:\n",
    "    ref, std = line['text1'], line['text2']\n",
    "    eval_list.append([ref, std])\n",
    "\n",
    "pred_result = {\n",
    "    'pred': [],\n",
    "    'pred_socres': []\n",
    "}\n",
    "for i in predictor(eval_list):\n",
    "    pred_result['pred'] = pred_result['pred'] + i['pred']\n",
    "    pred_result['pred_socres'] = pred_result['pred_socres'] + i['pred_socres']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_list = []\n",
    "\n",
    "for idx, line in enumerate(data):\n",
    "    label = int(line['label'])\n",
    "    if pred_result['pred'][idx] != label:\n",
    "        line['pred_label'] = pred_result['pred'][idx]\n",
    "        line['pred_score'] = pred_result['pred_socres'][idx]\n",
    "        N_list.append(line)\n",
    "\n",
    "with open('./data_record/STS_CNSTS_bert/CHIP-STS_N.json', 'w+') as f:\n",
    "    json.dump(N_list, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算实体向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertConfig, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('/home/lpc/models/text2vec-base-chinese/')\n",
    "model = BertModel.from_pretrained('/home/lpc/models/text2vec-base-chinese/')\n",
    "DATASET = 'CNSTS'\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "with open('./dataset/CHIP-STS/ner_final') as f:\n",
    "    ori_list = f.read().split('\\n')\n",
    "\n",
    "if ori_list[-1] == '':\n",
    "    ori_list = ori_list[:-1]\n",
    "\n",
    "result_list = []\n",
    "\n",
    "result_list.append([0 for _ in range(768)])\n",
    "\n",
    "num_batches = len(ori_list) / BATCH_SIZE if len(ori_list) % BATCH_SIZE == 0 else int(len(ori_list) / BATCH_SIZE + 1)\n",
    "\n",
    "for idx in tqdm(range(num_batches)):\n",
    "    lines = ori_list[idx*BATCH_SIZE : (idx+1)*BATCH_SIZE]\n",
    "    entity_list = [line.split(',')[0] for line in lines]\n",
    "    T = tokenizer(entity_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    T = {k: v.cuda() for k, v in T.items()}\n",
    "    output = model(**T)\n",
    "    result_list += output.pooler_output.tolist()\n",
    "\n",
    "if not os.path.exists(f'./embedding/{DATASET}'):\n",
    "    os.mkdir(f'./embedding/{DATASET}')\n",
    "\n",
    "result_list_num = np.array(result_list)\n",
    "\n",
    "with open(f'./embedding/{DATASET}/ori_{DATASET}.numpy', 'wb') as f:\n",
    "    pickle.dump(result_list_num, f, 2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出负样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./data_record/CLS_CNSTSAC_acbert_simcse_entity/predict_gold.csv') as f:\n",
    "    predict_label = f.read().split('\\n')\n",
    "\n",
    "if predict_label[-1] == '':\n",
    "    predict_label = predict_label[:-1]\n",
    "\n",
    "predict_label = [item.split('\\t') for item in predict_label]\n",
    "\n",
    "with open('./dataset/CHIP-STS/dev.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "result = []\n",
    "for idx, line in enumerate(data):\n",
    "    if float(predict_label[idx][0]) != float(predict_label[idx][1]):\n",
    "        result.append(line)\n",
    "\n",
    "with open('./dataset/CHIP-STS/CHIP-STS_N.json', 'w+') as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcpower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
