{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析句子长度对模型性能的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def RMSE(x, y):\n",
    "    return (x - y) ** 2\n",
    "\n",
    "\n",
    "with open('./dataset/CHIP-STS/CHIP-STS_dev.json') as f:\n",
    "    ori_list = f.read()\n",
    "ori_list = json.loads(ori_list)\n",
    "\n",
    "with open('./data_record/SAS_CNSTSX_r2bert/predict_gold.csv') as f:\n",
    "    pred_gold_list = f.read()\n",
    "    pred_gold_list = pred_gold_list.split('\\n')\n",
    "    if pred_gold_list[-1] == '':\n",
    "        pred_gold_list = pred_gold_list[:-1]\n",
    "\n",
    "slot = {\n",
    "    # 10: {\n",
    "    #     'RMSE': [],\n",
    "    #     'count': 0\n",
    "    # },\n",
    "    25: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    },\n",
    "    50: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    },\n",
    "    75: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    },\n",
    "    100: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    },\n",
    "    125: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    },\n",
    "    150: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    },\n",
    "    175: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    },\n",
    "    200: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    },\n",
    "    999: {\n",
    "        'RMSE': [],\n",
    "        'count': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "for idx, item in enumerate(ori_list):\n",
    "    ans = item['text2']\n",
    "    gold, pred = pred_gold_list[idx].split('\\t')\n",
    "    length = len(ans)\n",
    "    for key in slot:\n",
    "        if length > key:\n",
    "            continue\n",
    "        else:\n",
    "            slot[key]['count'] += 1\n",
    "            slot[key]['RMSE'].append(RMSE(float(pred), float(gold)))\n",
    "            break\n",
    "\n",
    "for key in slot:\n",
    "    print('<={}: count: {}, rmse: {:.3f}'.format(key, slot[key]['count'], math.sqrt(np.mean(slot[key]['RMSE']))))\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析不同评分对模型性能的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def RMSE(x, y):\n",
    "    return (x - y) ** 2\n",
    "\n",
    "def print_model_rmse_in_gold_dis(model_list=[], dataset_name='SAS'):\n",
    "    for model in model_list:\n",
    "        with open('./data_record/SAS_{}_{}/predict_gold.csv'.format(dataset_name, model)) as f:\n",
    "            pred_gold_list = f.read()\n",
    "            pred_gold_list = pred_gold_list.split('\\n')\n",
    "            if pred_gold_list[-1] == '':\n",
    "                pred_gold_list = pred_gold_list[:-1]\n",
    "\n",
    "        slot = {\n",
    "            0.2: {\n",
    "                'RMSE': [],\n",
    "                'count': 0\n",
    "            },\n",
    "            0.4: {\n",
    "                'RMSE': [],\n",
    "                'count': 0\n",
    "            },\n",
    "            0.6: {\n",
    "                'RMSE': [],\n",
    "                'count': 0\n",
    "            },\n",
    "            0.8: {\n",
    "                'RMSE': [],\n",
    "                'count': 0\n",
    "            },\n",
    "            1.2: {\n",
    "                'RMSE': [],\n",
    "                'count': 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for idx, line in enumerate(pred_gold_list):\n",
    "            pred, gold = pred_gold_list[idx].split('\\t')\n",
    "            for key in slot:\n",
    "                if float(gold) > key:\n",
    "                    continue\n",
    "                else:\n",
    "                    slot[key]['count'] += 1\n",
    "                    slot[key]['RMSE'].append(RMSE(float(pred), float(gold)))\n",
    "                    break\n",
    "        \n",
    "        print('{}:'.format(model))\n",
    "        for key in slot:\n",
    "            print('<={}: count: {}, rmse: {:.3f}'.format(key, slot[key]['count'], math.sqrt(np.mean(slot[key]['RMSE']))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_rmse_in_gold_dis(['textcnn', 'bert', 'bimpm', 'esim', 'sbert', 'r2bert'], 'ASAG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_rmse_in_gold_dis(['bert'], 'SFR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算模型分类性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def F1(X, Y):\n",
    "    result_dict = {}\n",
    "    for idx, _ in enumerate(X):\n",
    "        x, y = X[idx], Y[idx]\n",
    "        if x not in result_dict:\n",
    "            result_dict[x] = {\n",
    "                'TP': 0,\n",
    "                'FP': 0,\n",
    "                'FN': 0\n",
    "            }\n",
    "        if y not in result_dict:\n",
    "            result_dict[y] = {\n",
    "                'TP': 0,\n",
    "                'FP': 0,\n",
    "                'FN': 0\n",
    "            }\n",
    "        if x == y:\n",
    "            result_dict[x]['TP'] += 1\n",
    "        else:\n",
    "            result_dict[x]['FP'] += 1\n",
    "            result_dict[y]['FN'] += 1\n",
    "\n",
    "    P = []\n",
    "    R = []\n",
    "    F1Score = []\n",
    "    for key in result_dict:\n",
    "        TP, FP, FN = result_dict[key]['TP'], result_dict[key]['FP'], result_dict[key]['FN']\n",
    "        if TP == 0:\n",
    "            continue\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        P.append(precision)\n",
    "        R.append(recall)\n",
    "        F1Score.append(f1)\n",
    "\n",
    "    return np.mean(P), np.mean(R), np.mean(F1Score)\n",
    "\n",
    "def map(score):\n",
    "    if score <= 3:\n",
    "        return 0\n",
    "    elif score <= 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "\n",
    "def print_model_f1(model_list=[], dataset_name='SAS'):\n",
    "    for model in model_list:\n",
    "        with open('./data_record/SAS_{}_{}/predict_gold{}.csv'.format(dataset_name, model, '_sota' if model == 'msim' else '')) as f:\n",
    "            pred_gold_list = f.read()\n",
    "            pred_gold_list = pred_gold_list.split('\\n')\n",
    "            if pred_gold_list[-1] == '':\n",
    "                pred_gold_list = pred_gold_list[:-1]\n",
    "\n",
    "        pred_list = []\n",
    "        gold_list = []\n",
    "\n",
    "        for line in pred_gold_list:\n",
    "            pred, gold = line.split('\\t')\n",
    "            pred = round(float(pred) * 10)\n",
    "            gold = round(float(gold) * 10)\n",
    "            pred_list.append(map(pred))\n",
    "            gold_list.append(map(gold))\n",
    "\n",
    "        result = F1(pred_list, gold_list)\n",
    "\n",
    "        print('{}\\n P: {:.2f}, R: {:.2f}, F1: {:.2f}\\n'.format(\n",
    "            model, result[0] * 100, result[1] * 100, result[2] * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_f1(['textcnn', 'esim', 'bimpm', 'bert', 'sbert', 'r2bert'], 'CNSTSX')\n",
    "print_model_f1(['msim'], 'CNSTSX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算数据集句向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertConfig, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "config = BertConfig.from_pretrained('./model/chinese_wwm_ext/bert_config.json')\n",
    "model = BertModel.from_pretrained('./model/chinese_wwm_ext/pytorch_model.bin', config=config)\n",
    "\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "with open('./dataset/CHIP-STS/CHIP-STS_train.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "ref_list = []\n",
    "std_list = []\n",
    "\n",
    "for idx, line in tqdm(enumerate(data)):\n",
    "    ref, std = line['text1'], line['text2']\n",
    "    ref_T = tokenizer(ref, return_tensors=\"pt\")\n",
    "    std_T = tokenizer(std, return_tensors=\"pt\")\n",
    "    ref_T = {k: v.cuda() for k, v in ref_T.items()}\n",
    "    std_T = {k: v.cuda() for k, v in std_T.items()}\n",
    "    output = model(**ref_T)\n",
    "    ref_list.append(output.pooler_output[0].tolist())\n",
    "    output = model(**std_T)\n",
    "    std_list.append(output.pooler_output[0].tolist())\n",
    "\n",
    "if not os.path.exists('./data_record/tSNE/CNSTS'):\n",
    "    os.mkdir('./data_record/tSNE/CNSTS')\n",
    "\n",
    "with open('./data_record/tSNE/CNSTS/ref.tsv', 'w+') as f:\n",
    "    for ref in ref_list:\n",
    "        f.write('\\t'.join([str(i) for i in ref]) + '\\n')\n",
    "\n",
    "with open('./data_record/tSNE/CNSTS/std.tsv', 'w+') as f:\n",
    "    for std in std_list:\n",
    "        f.write('\\t'.join(str(i) for i in std) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STS样例预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpc/miniconda3/envs/pcpower/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1642: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoModel Choose Model: bert\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing Resume PATH: ./save_model/STS_CNSTS_bert/bert/bert_2000.pth ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpc/.local/lib/python3.8/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'CC.models.bert.Bert' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from CC.predictor import Predictor\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./model/chinese_wwm_ext/vocab.txt')\n",
    "predictor = Predictor(tokenizer, model_name=\"bert\",  padding_length=150, resume_path='./save_model/STS_CNSTS_bert/bert/bert_2000.pth', batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:29<00:00,  8.52it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('./dataset/CHIP-STS/CHIP-STS_dev.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "eval_list = []\n",
    "for line in data:\n",
    "    ref, std = line['text1'], line['text2']\n",
    "    eval_list.append([ref, std])\n",
    "\n",
    "pred_result = {\n",
    "    'pred': [],\n",
    "    'pred_socres': []\n",
    "}\n",
    "for i in predictor(eval_list):\n",
    "    pred_result['pred'] = pred_result['pred'] + i['pred']\n",
    "    pred_result['pred_socres'] = pred_result['pred_socres'] + i['pred_socres']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_list = []\n",
    "\n",
    "for idx, line in enumerate(data):\n",
    "    label = int(line['label'])\n",
    "    if pred_result['pred'][idx] != label:\n",
    "        line['pred_label'] = pred_result['pred'][idx]\n",
    "        line['pred_score'] = pred_result['pred_socres'][idx]\n",
    "        N_list.append(line)\n",
    "\n",
    "with open('./data_record/STS_CNSTS_bert/CHIP-STS_N.json', 'w+') as f:\n",
    "    json.dump(N_list, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e1e097b6c3c5a2a39328ddbc7de6327b7bd71c15618bc750f041eecacee4167"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pcpower')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
